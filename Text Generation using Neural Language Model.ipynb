{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1596871317154,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Z_CCxOEI4iZK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4339,
     "status": "ok",
     "timestamp": 1596871317159,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "ypl18CrLFmt6",
    "outputId": "b4199d46-a7a3-4e1f-f2d5-94fac4bac47c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20b8fd99470>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproducing same results\n",
    "SEED = 1\n",
    "\n",
    "# torch\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_5gPvXxWjru"
   },
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data\n",
    "with open(\"Dailog-dataset.dialogs_dataset\", \"rb\") as f:\n",
    "  dialogs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1596871317163,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "81_SXWZlE6Zb",
    "outputId": "eff429cf-724e-4653-cdda-46068a48035f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64776"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of text sequences\n",
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1596871317886,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "zSyAzbttAqP2",
    "outputId": "7b8a980c-cfda-4327-eaaa-98f431ad4249"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What about whole?',\n",
       " ' Let me send you the cupon',\n",
       " \"Hi I'd like two tickets to see Us\",\n",
       " \"Well let's go for that option instead\",\n",
       " 'Darn, ok what else looks close to that?',\n",
       " \" Isn't the UberX available?\",\n",
       " ' No extra flavorings',\n",
       " 'And that should do me',\n",
       " 'wow ok i guess',\n",
       " 'That sounds great']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 random dialogs\n",
    "random.sample(dialogs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGGTGRUDW9I3"
   },
   "source": [
    "# 3. Preprocessing and Exploring Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crvUl_ngM8sb"
   },
   "source": [
    "## 3.1 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1535,
     "status": "ok",
     "timestamp": 1596871320701,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "kUuUKZgUFhkl"
   },
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "dialogs_clean = []\n",
    "\n",
    "for i in dialogs:\n",
    "  # remove everything except alphabets\n",
    "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "  # convert text to lowercase\n",
    "  i = i.lower()\n",
    "  # add cleaned text to the list\n",
    "  dialogs_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1596871320704,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "DgZwFK1eSjkN",
    "outputId": "751d4097-be01-43a7-d25f-3466b7fbfe6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hey there i'm looking to make reservations for a table at a mexican restaurant in wasilla\",\n",
       " 'no that should do it',\n",
       " 'a two liter of pepsi',\n",
       " 'it is just me and my girlfriend',\n",
       " 'how much is the service going to cost',\n",
       " 'hey i need an ubur',\n",
       " 'hi i would like a coffee from starbucks',\n",
       " ' add that to the list as well',\n",
       " 'do you want to go and try and wait',\n",
       " \"how about one o'clock sharp\"]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(dialogs_clean, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYDzFQVvNA7_"
   },
   "source": [
    "\n",
    "## 3.2 Finding Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1596871322317,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "V1hrCYRp11UI"
   },
   "outputs": [],
   "source": [
    "# get list of all the words\n",
    "all_words = \" \".join(dialogs_clean).split()\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "# add word-count pair to the dictionary\n",
    "for word in all_words:   \n",
    "  # check if the word is already in dictionary \n",
    "  if word in words_dict:\n",
    "    # increment count of word by 1 \n",
    "    words_dict[word] = words_dict[word] + 1\n",
    "  else:\n",
    "    # add the word to dictionary with count 1 \n",
    "    words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1596871322775,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "gNxSGPubWaqA"
   },
   "outputs": [],
   "source": [
    "# prepare a dataframe\n",
    "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
    "\n",
    "# sort words by their count in increasing order\n",
    "words_df = words_df.sort_values(by = ['count'])\n",
    "\n",
    "# reset dataframe index\n",
    "words_df.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1596871323200,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "KVPbPsSWo-Ak",
    "outputId": "ab5dbe60-c9fe-4ed4-c09e-d44c121f78f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1298,
     "status": "ok",
     "timestamp": 1596871323813,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "JTwmmOiEXBHt",
    "outputId": "fc5963db-838a-4f5d-ec13-846db582797a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppermiddle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shoots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andrea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0  uppermiddle      1\n",
       "1       shoots      1\n",
       "2        geesh      1\n",
       "3       andrea      1\n",
       "4      precice      1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1596871323815,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "EWJEto8TMPiq",
    "outputId": "a9deb629-e282-46e3-e99a-e2094b3473e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11142</th>\n",
       "      <td>you</td>\n",
       "      <td>11909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11143</th>\n",
       "      <td>a</td>\n",
       "      <td>13380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>to</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>the</td>\n",
       "      <td>15406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>i</td>\n",
       "      <td>19654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "11142  you  11909\n",
       "11143    a  13380\n",
       "11144   to  14000\n",
       "11145  the  15406\n",
       "11146    i  19654"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTcWd-pYFRob"
   },
   "source": [
    "## 3.3 Find and Replace Rare Words with \"Unknown\" Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1596871325920,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "1iC4ztG3XIP3"
   },
   "outputs": [],
   "source": [
    "# user specified threshold value\n",
    "rare_thresh = 4\n",
    "\n",
    "# get percentage of rare words in the vocabulary\n",
    "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
    "total_words = len(words_df) \n",
    "rare_dist = rare_words_count / total_words\n",
    "\n",
    "# coverage percentage of rare words in the corpus\n",
    "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1596871325922,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "helYHQ4BXNK9",
    "outputId": "71e44117-e1e5-4127-c876-8e75e3390988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words distribution in the vocabulary: 69.03\n",
      "Rare words coverage in the corpus: 2.27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
    "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1397,
     "status": "ok",
     "timestamp": 1596871326714,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "yJhbRQllXQJk"
   },
   "outputs": [],
   "source": [
    "# extract rare words in a list\n",
    "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "92d7ebd42a4e463eaed43f3960cfe36c",
      "1eccf91efb3641ab93437844fb3d8515",
      "b00386afebe447d989c1d40fc75ebb24",
      "bb3bafbb85f14bd0a9904c4f08d40946",
      "433348a8133446d0a9a4dc38f028bdc6",
      "c684ec2cc5e9413a822c072387aeedd5",
      "233b3deec9394d288ed235c2054abe08",
      "cbb68ce201524cfa8eafec85788f9c42"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19783,
     "status": "ok",
     "timestamp": 1596871347751,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "hEn-L1_8YBjl",
    "outputId": "a0977257-41ce-4fc7-eb66-1190f88e5b00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartik Kotian\\AppData\\Local\\Temp\\ipykernel_17668\\2064763968.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for d in tqdm_notebook(dialogs_clean):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5772b3413ffa4d729589c4033710e853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
    "pattern = \"\"\n",
    "for i in rare_words:\n",
    "  pattern+= \" {} |\".format(i)\n",
    "\n",
    "# removing the last element which is \"|\"\n",
    "pattern = pattern[:-1]\n",
    "\n",
    "# empty list \n",
    "dialogs_clean_v2 = []\n",
    "\n",
    "# replace the rare words with the <unk> token\n",
    "for d in tqdm_notebook(dialogs_clean):\n",
    "  text = re.sub(pattern, \" <unk> \", d)\n",
    "  dialogs_clean_v2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19223,
     "status": "ok",
     "timestamp": 1596871347752,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "SoQwO5FAZSP1",
    "outputId": "8caaf8a6-b6dd-4046-c1c0-80420e4dbaa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does it serve traditional chinese dessert',\n",
       " 'how much extra time to reach <unk> ',\n",
       " 'ok lets reserve a table for dinner at hakkasan',\n",
       " 'hello i need to get a car please',\n",
       " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
       " 'bowling alley <unk> highway <unk> park il',\n",
       " 'what types of cars does uber have',\n",
       " \"what's the price difference\",\n",
       " 'ok get me the cheapest please',\n",
       " 'ok then get me the next level']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_clean_v2[520:530]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waTukoy1AbVT"
   },
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SvIJQ0brBWW"
   },
   "source": [
    "## 4.1 Prepare Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17411,
     "status": "ok",
     "timestamp": 1596871347754,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "H5BYGcwBF7hX",
    "outputId": "2559acd7-fec8-4b47-f91f-40a3857f0637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# capture length of all the sequences\n",
    "text_word_count = []\n",
    "for i in dialogs_clean_v2:\n",
    "  text_word_count.append(len(i.split()))\n",
    "        \n",
    "# plot the sequence lengths\n",
    "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16835,
     "status": "ok",
     "timestamp": 1596871347755,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "uw8F6bfwjBeo"
   },
   "outputs": [],
   "source": [
    "# function to create sequences of equal length\n",
    "def create_seq(text, seq_len = 5):\n",
    "      \n",
    "  sequences = []    \n",
    "  \n",
    "  if len(text.split()) > seq_len:\n",
    "    for i in range(seq_len, len(text.split())):\n",
    "      # select sequence of tokens\n",
    "      seq = text.split()[i-seq_len:i+1]\n",
    "      # append sequence to the list\n",
    "      sequences.append(\" \".join(seq))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "  else:\n",
    "    \n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16794,
     "status": "ok",
     "timestamp": 1596871348315,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "p0psJCgo9QtH"
   },
   "outputs": [],
   "source": [
    "# create sequences of equal length\n",
    "seqs = [create_seq(i) for i in dialogs_clean_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16070,
     "status": "ok",
     "timestamp": 1596871348316,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "LCrf5t5vCI6I",
    "outputId": "8c4b8d92-3128-467d-9c60-eab93af446d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"hi i'm looking to book a\",\n",
       "  \"i'm looking to book a table\",\n",
       "  'looking to book a table for',\n",
       "  'to book a table for korean',\n",
       "  'book a table for korean fod'],\n",
       " ['somewhere in southern nyc maybe the',\n",
       "  'in southern nyc maybe the east',\n",
       "  'southern nyc maybe the east village'],\n",
       " [\"we don't want to sit at\",\n",
       "  \"don't want to sit at the\",\n",
       "  'want to sit at the bar',\n",
       "  'to sit at the bar but',\n",
       "  'sit at the bar but anywhere',\n",
       "  'at the bar but anywhere else',\n",
       "  'the bar but anywhere else is',\n",
       "  'bar but anywhere else is fine'],\n",
       " ['what times are available'],\n",
       " [\"yikes we can't do those times\"],\n",
       " ['let me check'],\n",
       " [\"great let's book that\"],\n",
       " [\"no that's it just book\"],\n",
       " ['hi i would like to see',\n",
       "  'i would like to see if',\n",
       "  'would like to see if the',\n",
       "  'like to see if the movie',\n",
       "  'to see if the movie what',\n",
       "  'see if the movie what men',\n",
       "  'if the movie what men want',\n",
       "  'the movie what men want is',\n",
       "  'movie what men want is playing',\n",
       "  'what men want is playing here'],\n",
       " ['yes for me and a friend',\n",
       "  'for me and a friend so',\n",
       "  'me and a friend so two',\n",
       "  'and a friend so two tickets',\n",
       "  'a friend so two tickets please']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65541,
     "status": "ok",
     "timestamp": 1596871398538,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "S4O3-S6PA78w"
   },
   "outputs": [],
   "source": [
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64651,
     "status": "ok",
     "timestamp": 1596871398543,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "dwmBsxuqxPRq",
    "outputId": "35a8e7c5-df33-411d-cb30-4c011f685d10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book a\",\n",
       " \"i'm looking to book a table\",\n",
       " 'looking to book a table for',\n",
       " 'to book a table for korean',\n",
       " 'book a table for korean fod',\n",
       " 'somewhere in southern nyc maybe the',\n",
       " 'in southern nyc maybe the east',\n",
       " 'southern nyc maybe the east village',\n",
       " \"we don't want to sit at\",\n",
       " \"don't want to sit at the\",\n",
       " 'want to sit at the bar',\n",
       " 'to sit at the bar but',\n",
       " 'sit at the bar but anywhere',\n",
       " 'at the bar but anywhere else',\n",
       " 'the bar but anywhere else is']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63767,
     "status": "ok",
     "timestamp": 1596871398546,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "cNW5MIpTDufa",
    "outputId": "311f64a5-6a15-47ec-cef7-521b49f62f5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205346"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62945,
     "status": "ok",
     "timestamp": 1596871398548,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "M0dppnffaEgG"
   },
   "outputs": [],
   "source": [
    "# create input and target sequences (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62137,
     "status": "ok",
     "timestamp": 1596871398550,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "27XKPYgLwuOF",
    "outputId": "995c0896-0844-4c50-f777-0a45651fc895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hi i'm looking to book\", \"i'm looking to book a\")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61130,
     "status": "ok",
     "timestamp": 1596871398552,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "rVge-c6WHUx1",
    "outputId": "e8de2d34-e100-4072-8f21-5bb9e96fe908"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to drive to several locations', 'drive to several locations do')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[88543], y[88543]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzR5aG9wrZJ5"
   },
   "source": [
    "## 4.2 Create Token-Integer Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59361,
     "status": "ok",
     "timestamp": 1596871398555,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "CSgaezAI60ht"
   },
   "outputs": [],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 1\n",
    "\n",
    "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58590,
     "status": "ok",
     "timestamp": 1596871398558,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "j5TSElOk_OQf",
    "outputId": "3eda0c7c-51e1-4792-d826-b441f5ad621c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5112, 'can')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2int[\"can\"], int2token[5112]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQOk80iBr51V"
   },
   "source": [
    "## 4.3 Split Data into Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56875,
     "status": "ok",
     "timestamp": 1596871398559,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Jn6a1aXvAcRl"
   },
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "# input sequences\n",
    "x_tr = x[:150000]\n",
    "x_val = x[150000:]\n",
    "\n",
    "# target sequences\n",
    "y_tr = y[:150000]\n",
    "y_val = y[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIfKsjE3sxgu"
   },
   "source": [
    "## 4.4 Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55912,
     "status": "ok",
     "timestamp": 1596871399525,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "QlG-sd79sNjH",
    "outputId": "912091b4-6c04-435c-efc0-af15733d22ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sequence length in train set\n",
    "text_word_count = []\n",
    "\n",
    "for i in x_tr:\n",
    "  text_word_count.append(len(i.split()))\n",
    "\n",
    "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55185,
     "status": "ok",
     "timestamp": 1596871399528,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "2umo7Mns5NKl"
   },
   "outputs": [],
   "source": [
    "# based on the plot above\n",
    "max_text_len = 5\n",
    "\n",
    "# function to perform padding\n",
    "def pad_sequence(seq, n):\n",
    "\n",
    "  # split input sequence into tokens\n",
    "  seq = seq.split()\n",
    "  \n",
    "  # check if no. of tokens in input sequence is less than 'n'\n",
    "  if len(seq) < n:\n",
    "    for i in range(n - len(seq)):\n",
    "      seq.append(\"<pad>\")\n",
    "\n",
    "  return \" \".join(seq)\n",
    "\n",
    "# pad text sequences (train set)\n",
    "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
    "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
    "\n",
    "# pad text sequences (validation set)\n",
    "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
    "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54370,
     "status": "ok",
     "timestamp": 1596871399530,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "gxwn3NyD-HBD",
    "outputId": "a42ed43e-ae07-43af-abc0-8cd67799a379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book\",\n",
       " \"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'somewhere in southern nyc maybe',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " \"we don't want to sit\",\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'what times are <pad> <pad>',\n",
       " \"yikes we can't do those\",\n",
       " 'let me <pad> <pad> <pad>',\n",
       " \"great let's book <pad> <pad>\"]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_padded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53577,
     "status": "ok",
     "timestamp": 1596871399532,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "PPEGI_Xf-ne_",
    "outputId": "337f39bb-63e0-4660-ce3b-d1c873a023d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'a table for korean fod',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " 'nyc maybe the east village',\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'but anywhere else is fine',\n",
       " 'times are available <pad> <pad>',\n",
       " \"we can't do those times\",\n",
       " 'me check <pad> <pad> <pad>',\n",
       " \"let's book that <pad> <pad>\"]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_padded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52801,
     "status": "ok",
     "timestamp": 1596871399533,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "mlUnd9v_-4ad"
   },
   "outputs": [],
   "source": [
    "# update mapping dictionaries\n",
    "int2token[0] = \"<pad>\"\n",
    "token2int[\"<pad>\"] = 0\n",
    "\n",
    "# set vocabulary size\n",
    "vocab_size = len(int2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLqEIRavtMpY"
   },
   "source": [
    "## 4.5 Convert Text Sequences to Integer Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51264,
     "status": "ok",
     "timestamp": 1596871399535,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "HVxwznePACXC"
   },
   "outputs": [],
   "source": [
    "# function to create integer sequences\n",
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51299,
     "status": "ok",
     "timestamp": 1596871400387,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "r7QVnobNARnr"
   },
   "outputs": [],
   "source": [
    "# convert text sequences to integer sequences\n",
    "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
    "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
    "\n",
    "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
    "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50390,
     "status": "ok",
     "timestamp": 1596871400388,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "qo8g1L_FwAOC",
    "outputId": "790f6769-2037-47b1-dda4-0b2787774789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3153, 5865, 5567, 2372, 1336],\n",
       " [5865, 5567, 2372, 1336, 1829],\n",
       " [5567, 2372, 1336, 1829, 5214],\n",
       " [2372, 1336, 1829, 5214, 1965],\n",
       " [1336, 1829, 5214, 1965, 3611],\n",
       " [840, 6073, 5384, 2622, 6062],\n",
       " [6073, 5384, 2622, 6062, 4545],\n",
       " [5384, 2622, 6062, 4545, 1164],\n",
       " [1850, 6312, 3014, 2372, 125],\n",
       " [6312, 3014, 2372, 125, 2944]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49598,
     "status": "ok",
     "timestamp": 1596871400390,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "yGa_ORxZwDkw",
    "outputId": "130c0343-d1f4-49c0-816d-796c3eccd77b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5865, 5567, 2372, 1336, 1829],\n",
       " [5567, 2372, 1336, 1829, 5214],\n",
       " [2372, 1336, 1829, 5214, 1965],\n",
       " [1336, 1829, 5214, 1965, 3611],\n",
       " [1829, 5214, 1965, 3611, 6215],\n",
       " [6073, 5384, 2622, 6062, 4545],\n",
       " [5384, 2622, 6062, 4545, 1164],\n",
       " [2622, 6062, 4545, 1164, 238],\n",
       " [6312, 3014, 2372, 125, 2944],\n",
       " [3014, 2372, 125, 2944, 4545]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48717,
     "status": "ok",
     "timestamp": 1596871400391,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "eLntLMs5NwGi",
    "outputId": "a11c9e89-ab24-48b0-f616-5e06ad13aae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert lists into numpy arrays\n",
    "x_tr_int = np.array(x_tr_int)\n",
    "y_tr_int = np.array(y_tr_int)\n",
    "\n",
    "x_val_int = np.array(x_val_int)\n",
    "y_val_int = np.array(y_val_int)\n",
    "\n",
    "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y3bZcFywH33"
   },
   "source": [
    "# 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCh2prnryopJ"
   },
   "source": [
    "## 5.1 Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38190,
     "status": "ok",
     "timestamp": 1596871400392,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "ldcidFnPRsOM"
   },
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "## embedding layer: \n",
    "##    input dim = vocab_size, \n",
    "##    ouput dim = 200\n",
    "\n",
    "## LSTM layer:\n",
    "##    input dim = 200\n",
    "##    hidden units = 256\n",
    "##    layers = 2\n",
    "##    output dim = 256\n",
    "\n",
    "## Dropout Layer\n",
    "##    input dim = 256\n",
    "##    output dim = 256\n",
    "\n",
    "## fully connected layer\n",
    "##    input dim = 256\n",
    "##    ouput dim = vocab_size\n",
    "\n",
    "class WordLSTM(nn.Module):\n",
    "      \n",
    "  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_layers = n_layers\n",
    "    self.n_hidden = n_hidden\n",
    "    self.lr = lr\n",
    "    \n",
    "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "    ## define the LSTM\n",
    "    # input data is of shape (batch size, sequence length, no. of features)...\n",
    "    # ...therefore we need batch_first=True\n",
    "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
    "    \n",
    "    ## define a dropout layer\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    ## define the fully-connected layer\n",
    "    self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "  \n",
    "  def forward(self, x, hidden):\n",
    "    ''' Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state is `hidden`. '''\n",
    "\n",
    "    ## pass input through embedding layer\n",
    "    embedded = self.emb_layer(x)     \n",
    "    \n",
    "    ## Get the outputs and the new hidden state from the lstm\n",
    "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "    \n",
    "    ## pass through a dropout layer\n",
    "    out = self.dropout(lstm_output)\n",
    "    \n",
    "    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)\n",
    "    out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "    ## put \"out\" through the fully-connected layer\n",
    "    out = self.fc(out)\n",
    "\n",
    "    # return the final output and the hidden state\n",
    "    return out, hidden\n",
    "    \n",
    "    \n",
    "  def init_hidden(self, batch_size):\n",
    "    ''' Initializes hidden state '''\n",
    "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "    # initialized to zero, for hidden state and cell state of LSTM\n",
    "    weight = next(self.parameters()).data\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14294,
     "status": "ok",
     "timestamp": 1596871400394,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "LQq72w9ADLn_",
    "outputId": "bf5e2652-1433-40c0-d0f0-e07e7fe6e79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(6502, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = WordLSTM()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13266,
     "status": "ok",
     "timestamp": 1596871400396,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "2pRtYvAVFCKB"
   },
   "outputs": [],
   "source": [
    "# function to generate batches\n",
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "  # iterate through the arrays\n",
    "  prv = 0\n",
    "  \n",
    "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "    # batch of input sequences\n",
    "    x = arr_x[prv:n,:]\n",
    "\n",
    "    # batch of target sequences\n",
    "    y = arr_y[prv:n,:]\n",
    "\n",
    "    prv = n\n",
    "    \n",
    "    yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpV3vbVeytW1"
   },
   "source": [
    "## 5.2 Start Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1132,
     "status": "ok",
     "timestamp": 1596871404096,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "QHMBy8HQ2GhY"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
    "      \n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  \n",
    "  # optimizer\n",
    "  opt = torch.optim.Adam(net.parameters(), lr=lr)   \n",
    "    \n",
    "    \n",
    "  # loss function\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "  \n",
    "  counter = 0\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for e in range(epochs):\n",
    "            \n",
    "\n",
    "    # iterate over batches\n",
    "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
    "      counter+= 1\n",
    "      \n",
    "      # convert arrays to tensors\n",
    "      inputs, targets = torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
    "      \n",
    "      if(torch.cuda.is_available()):\n",
    "        # push tensors to GPU\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "      # initialize hidden state\n",
    "      h = net.init_hidden(batch_size)\n",
    "\n",
    "      # set accumulated gradients to zero\n",
    "      net.zero_grad()\n",
    "      \n",
    "      # get the output from the model\n",
    "      output, h = net(inputs, h)\n",
    "      \n",
    "      # calculate the loss and perform backprop\n",
    "      loss = criterion(output, targets.view(-1))\n",
    "      loss.backward()\n",
    "      \n",
    "      opt.step()\n",
    "      \n",
    "      if counter % print_every == 0:\n",
    "        # Get validation loss\n",
    "        \n",
    "        val_losses = []\n",
    "\n",
    "        net.eval()\n",
    "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
    "            \n",
    "          x, y = torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
    "          \n",
    "          val_h = net.init_hidden(batch_size)\n",
    "\n",
    "          inputs, targets = x, y\n",
    "          if(torch.cuda.is_available()):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "          output, val_h = net(inputs, val_h)\n",
    "\n",
    "          val_loss = criterion(output, targets.view(-1))\n",
    "          val_losses.append(val_loss.item())\n",
    "\n",
    "        #save the best model\n",
    "        if np.mean(val_losses) < best_valid_loss:\n",
    "          best_valid_loss = np.mean(val_losses)\n",
    "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "      \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.4f}...\".format(loss.item()),\n",
    "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739047,
     "status": "ok",
     "timestamp": 1596872148847,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "sbGexKELaTNb",
    "outputId": "ddbf5b26-2644-4356-dcaa-f8a17dc72f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 32... Loss: 5.6857... ppl: 359.5199  Val Loss: 5.8848\n",
      "Epoch: 1/10... Step: 64... Loss: 5.4815... ppl: 281.7055  Val Loss: 5.6409\n",
      "Epoch: 1/10... Step: 96... Loss: 5.9103... ppl: 238.6936  Val Loss: 5.4752\n",
      "Epoch: 1/10... Step: 128... Loss: 5.1021... ppl: 207.6404  Val Loss: 5.3358\n",
      "Epoch: 1/10... Step: 160... Loss: 6.1976... ppl: 183.3769  Val Loss: 5.2115\n",
      "Epoch: 1/10... Step: 192... Loss: 5.6389... ppl: 163.3888  Val Loss: 5.0961\n",
      "Epoch: 1/10... Step: 224... Loss: 5.0784... ppl: 147.9915  Val Loss: 4.9972\n",
      "Epoch: 1/10... Step: 256... Loss: 4.1131... ppl: 135.7399  Val Loss: 4.9107\n",
      "Epoch: 1/10... Step: 288... Loss: 4.8374... ppl: 124.9900  Val Loss: 4.8282\n",
      "Epoch: 1/10... Step: 320... Loss: 4.7748... ppl: 116.6690  Val Loss: 4.7593\n",
      "Epoch: 1/10... Step: 352... Loss: 4.2494... ppl: 109.6019  Val Loss: 4.6969\n",
      "Epoch: 1/10... Step: 384... Loss: 4.9617... ppl: 103.0241  Val Loss: 4.6350\n",
      "Epoch: 1/10... Step: 416... Loss: 4.7968... ppl: 98.5184  Val Loss: 4.5902\n",
      "Epoch: 1/10... Step: 448... Loss: 4.3905... ppl: 94.5653  Val Loss: 4.5493\n",
      "Epoch: 1/10... Step: 480... Loss: 4.2736... ppl: 91.0498  Val Loss: 4.5114\n",
      "Epoch: 1/10... Step: 512... Loss: 4.3021... ppl: 87.3567  Val Loss: 4.4700\n",
      "Epoch: 1/10... Step: 544... Loss: 4.5782... ppl: 84.5012  Val Loss: 4.4368\n",
      "Epoch: 1/10... Step: 576... Loss: 5.4962... ppl: 81.9122  Val Loss: 4.4056\n",
      "Epoch: 1/10... Step: 608... Loss: 4.4150... ppl: 79.5279  Val Loss: 4.3761\n",
      "Epoch: 1/10... Step: 640... Loss: 4.3843... ppl: 77.0713  Val Loss: 4.3447\n",
      "Epoch: 1/10... Step: 672... Loss: 4.3631... ppl: 75.6474  Val Loss: 4.3261\n",
      "Epoch: 1/10... Step: 704... Loss: 4.1631... ppl: 73.5886  Val Loss: 4.2985\n",
      "Epoch: 1/10... Step: 736... Loss: 4.6974... ppl: 72.2401  Val Loss: 4.2800\n",
      "Epoch: 1/10... Step: 768... Loss: 3.9077... ppl: 70.1232  Val Loss: 4.2503\n",
      "Epoch: 1/10... Step: 800... Loss: 4.1120... ppl: 68.9656  Val Loss: 4.2336\n",
      "Epoch: 1/10... Step: 832... Loss: 4.6264... ppl: 67.8268  Val Loss: 4.2170\n",
      "Epoch: 1/10... Step: 864... Loss: 4.1466... ppl: 66.5914  Val Loss: 4.1986\n",
      "Epoch: 1/10... Step: 896... Loss: 4.2246... ppl: 65.3927  Val Loss: 4.1804\n",
      "Epoch: 1/10... Step: 928... Loss: 4.2698... ppl: 63.7329  Val Loss: 4.1547\n",
      "Epoch: 1/10... Step: 960... Loss: 3.8761... ppl: 63.4662  Val Loss: 4.1505\n",
      "Epoch: 1/10... Step: 992... Loss: 3.7785... ppl: 62.0050  Val Loss: 4.1272\n",
      "Epoch: 1/10... Step: 1024... Loss: 3.5462... ppl: 61.0800  Val Loss: 4.1122\n",
      "Epoch: 1/10... Step: 1056... Loss: 4.1985... ppl: 60.1142  Val Loss: 4.0962\n",
      "Epoch: 1/10... Step: 1088... Loss: 3.4022... ppl: 59.1811  Val Loss: 4.0806\n",
      "Epoch: 1/10... Step: 1120... Loss: 4.0937... ppl: 58.5945  Val Loss: 4.0706\n",
      "Epoch: 1/10... Step: 1152... Loss: 3.1699... ppl: 57.9643  Val Loss: 4.0598\n",
      "Epoch: 1/10... Step: 1184... Loss: 3.9849... ppl: 57.1255  Val Loss: 4.0453\n",
      "Epoch: 1/10... Step: 1216... Loss: 3.7874... ppl: 56.5032  Val Loss: 4.0343\n",
      "Epoch: 1/10... Step: 1248... Loss: 4.0947... ppl: 55.7161  Val Loss: 4.0203\n",
      "Epoch: 1/10... Step: 1280... Loss: 4.2437... ppl: 55.0583  Val Loss: 4.0084\n",
      "Epoch: 1/10... Step: 1312... Loss: 4.8310... ppl: 54.8216  Val Loss: 4.0041\n",
      "Epoch: 1/10... Step: 1344... Loss: 3.8036... ppl: 54.2365  Val Loss: 3.9934\n",
      "Epoch: 1/10... Step: 1376... Loss: 4.1513... ppl: 53.3175  Val Loss: 3.9763\n",
      "Epoch: 1/10... Step: 1408... Loss: 3.5500... ppl: 52.7699  Val Loss: 3.9659\n",
      "Epoch: 1/10... Step: 1440... Loss: 4.0628... ppl: 52.3250  Val Loss: 3.9575\n",
      "Epoch: 1/10... Step: 1472... Loss: 3.4098... ppl: 51.9543  Val Loss: 3.9504\n",
      "Epoch: 1/10... Step: 1504... Loss: 4.1486... ppl: 51.7319  Val Loss: 3.9461\n",
      "Epoch: 1/10... Step: 1536... Loss: 4.7834... ppl: 51.4327  Val Loss: 3.9403\n",
      "Epoch: 1/10... Step: 1568... Loss: 3.9126... ppl: 50.3566  Val Loss: 3.9191\n",
      "Epoch: 1/10... Step: 1600... Loss: 3.6786... ppl: 50.0954  Val Loss: 3.9139\n",
      "Epoch: 1/10... Step: 1632... Loss: 5.1517... ppl: 49.3304  Val Loss: 3.8985\n",
      "Epoch: 1/10... Step: 1664... Loss: 4.0250... ppl: 48.9222  Val Loss: 3.8902\n",
      "Epoch: 1/10... Step: 1696... Loss: 3.5660... ppl: 48.7879  Val Loss: 3.8875\n",
      "Epoch: 1/10... Step: 1728... Loss: 3.6625... ppl: 48.2251  Val Loss: 3.8759\n",
      "Epoch: 1/10... Step: 1760... Loss: 4.4822... ppl: 48.3370  Val Loss: 3.8782\n",
      "Epoch: 1/10... Step: 1792... Loss: 3.3939... ppl: 47.8456  Val Loss: 3.8680\n",
      "Epoch: 1/10... Step: 1824... Loss: 4.4488... ppl: 48.0103  Val Loss: 3.8714\n",
      "Epoch: 1/10... Step: 1856... Loss: 5.7679... ppl: 47.1330  Val Loss: 3.8530\n",
      "Epoch: 1/10... Step: 1888... Loss: 3.5696... ppl: 46.3760  Val Loss: 3.8368\n",
      "Epoch: 1/10... Step: 1920... Loss: 3.3826... ppl: 46.1269  Val Loss: 3.8314\n",
      "Epoch: 1/10... Step: 1952... Loss: 2.9596... ppl: 45.9024  Val Loss: 3.8265\n",
      "Epoch: 1/10... Step: 1984... Loss: 2.8883... ppl: 45.9006  Val Loss: 3.8265\n",
      "Epoch: 1/10... Step: 2016... Loss: 3.5971... ppl: 45.3742  Val Loss: 3.8149\n",
      "Epoch: 1/10... Step: 2048... Loss: 4.0756... ppl: 45.0897  Val Loss: 3.8087\n",
      "Epoch: 1/10... Step: 2080... Loss: 3.7383... ppl: 44.9007  Val Loss: 3.8045\n",
      "Epoch: 1/10... Step: 2112... Loss: 4.7243... ppl: 44.8819  Val Loss: 3.8040\n",
      "Epoch: 1/10... Step: 2144... Loss: 3.5078... ppl: 44.6218  Val Loss: 3.7982\n",
      "Epoch: 1/10... Step: 2176... Loss: 5.0977... ppl: 44.3584  Val Loss: 3.7923\n",
      "Epoch: 1/10... Step: 2208... Loss: 4.0632... ppl: 44.1816  Val Loss: 3.7883\n",
      "Epoch: 1/10... Step: 2240... Loss: 3.4955... ppl: 44.1132  Val Loss: 3.7868\n",
      "Epoch: 1/10... Step: 2272... Loss: 3.9256... ppl: 43.5520  Val Loss: 3.7740\n",
      "Epoch: 1/10... Step: 2304... Loss: 3.2908... ppl: 43.2187  Val Loss: 3.7663\n",
      "Epoch: 1/10... Step: 2336... Loss: 4.1701... ppl: 43.1106  Val Loss: 3.7638\n",
      "Epoch: 2/10... Step: 2368... Loss: 3.1850... ppl: 43.7630  Val Loss: 3.7788\n",
      "Epoch: 2/10... Step: 2400... Loss: 3.4020... ppl: 43.4111  Val Loss: 3.7707\n",
      "Epoch: 2/10... Step: 2432... Loss: 3.7172... ppl: 43.3776  Val Loss: 3.7699\n",
      "Epoch: 2/10... Step: 2464... Loss: 4.2611... ppl: 43.0506  Val Loss: 3.7624\n",
      "Epoch: 2/10... Step: 2496... Loss: 4.0226... ppl: 42.6988  Val Loss: 3.7542\n",
      "Epoch: 2/10... Step: 2528... Loss: 3.4864... ppl: 42.5475  Val Loss: 3.7506\n",
      "Epoch: 2/10... Step: 2560... Loss: 5.1810... ppl: 42.4339  Val Loss: 3.7479\n",
      "Epoch: 2/10... Step: 2592... Loss: 4.0031... ppl: 42.4470  Val Loss: 3.7483\n",
      "Epoch: 2/10... Step: 2624... Loss: 3.1542... ppl: 42.2631  Val Loss: 3.7439\n",
      "Epoch: 2/10... Step: 2656... Loss: 3.7766... ppl: 42.1028  Val Loss: 3.7401\n",
      "Epoch: 2/10... Step: 2688... Loss: 3.3562... ppl: 41.7945  Val Loss: 3.7328\n",
      "Epoch: 2/10... Step: 2720... Loss: 3.7185... ppl: 41.4958  Val Loss: 3.7256\n",
      "Epoch: 2/10... Step: 2752... Loss: 3.5902... ppl: 41.5724  Val Loss: 3.7274\n",
      "Epoch: 2/10... Step: 2784... Loss: 4.1616... ppl: 41.3777  Val Loss: 3.7227\n",
      "Epoch: 2/10... Step: 2816... Loss: 4.0890... ppl: 41.3755  Val Loss: 3.7227\n",
      "Epoch: 2/10... Step: 2848... Loss: 3.5305... ppl: 40.9649  Val Loss: 3.7127\n",
      "Epoch: 2/10... Step: 2880... Loss: 4.1765... ppl: 40.9009  Val Loss: 3.7112\n",
      "Epoch: 2/10... Step: 2912... Loss: 3.2220... ppl: 40.5309  Val Loss: 3.7021\n",
      "Epoch: 2/10... Step: 2944... Loss: 3.3806... ppl: 40.8090  Val Loss: 3.7089\n",
      "Epoch: 2/10... Step: 2976... Loss: 3.2984... ppl: 40.3620  Val Loss: 3.6979\n",
      "Epoch: 2/10... Step: 3008... Loss: 4.1257... ppl: 40.3132  Val Loss: 3.6967\n",
      "Epoch: 2/10... Step: 3040... Loss: 3.5090... ppl: 40.1486  Val Loss: 3.6926\n",
      "Epoch: 2/10... Step: 3072... Loss: 3.5969... ppl: 40.0954  Val Loss: 3.6913\n",
      "Epoch: 2/10... Step: 3104... Loss: 3.6996... ppl: 39.8874  Val Loss: 3.6861\n",
      "Epoch: 2/10... Step: 3136... Loss: 4.1049... ppl: 39.6818  Val Loss: 3.6809\n",
      "Epoch: 2/10... Step: 3168... Loss: 3.4492... ppl: 39.7294  Val Loss: 3.6821\n",
      "Epoch: 2/10... Step: 3200... Loss: 3.7746... ppl: 39.8785  Val Loss: 3.6858\n",
      "Epoch: 2/10... Step: 3232... Loss: 4.2324... ppl: 39.6149  Val Loss: 3.6792\n",
      "Epoch: 2/10... Step: 3264... Loss: 2.8644... ppl: 39.3134  Val Loss: 3.6716\n",
      "Epoch: 2/10... Step: 3296... Loss: 4.1940... ppl: 39.3269  Val Loss: 3.6719\n",
      "Epoch: 2/10... Step: 3328... Loss: 3.9911... ppl: 39.1483  Val Loss: 3.6674\n",
      "Epoch: 2/10... Step: 3360... Loss: 3.7045... ppl: 38.8454  Val Loss: 3.6596\n",
      "Epoch: 2/10... Step: 3392... Loss: 3.2337... ppl: 39.2894  Val Loss: 3.6710\n",
      "Epoch: 2/10... Step: 3424... Loss: 4.2005... ppl: 38.7859  Val Loss: 3.6581\n",
      "Epoch: 2/10... Step: 3456... Loss: 3.3889... ppl: 38.5849  Val Loss: 3.6529\n",
      "Epoch: 2/10... Step: 3488... Loss: 3.5453... ppl: 38.8034  Val Loss: 3.6585\n",
      "Epoch: 2/10... Step: 3520... Loss: 3.4025... ppl: 38.3456  Val Loss: 3.6466\n",
      "Epoch: 2/10... Step: 3552... Loss: 4.0061... ppl: 38.5127  Val Loss: 3.6510\n",
      "Epoch: 2/10... Step: 3584... Loss: 2.9389... ppl: 38.3199  Val Loss: 3.6460\n",
      "Epoch: 2/10... Step: 3616... Loss: 3.6099... ppl: 38.1934  Val Loss: 3.6427\n",
      "Epoch: 2/10... Step: 3648... Loss: 3.2093... ppl: 38.2021  Val Loss: 3.6429\n",
      "Epoch: 2/10... Step: 3680... Loss: 3.7622... ppl: 38.1593  Val Loss: 3.6418\n",
      "Epoch: 2/10... Step: 3712... Loss: 3.5125... ppl: 37.9577  Val Loss: 3.6365\n",
      "Epoch: 2/10... Step: 3744... Loss: 3.4474... ppl: 37.7928  Val Loss: 3.6321\n",
      "Epoch: 2/10... Step: 3776... Loss: 3.9813... ppl: 37.9198  Val Loss: 3.6355\n",
      "Epoch: 2/10... Step: 3808... Loss: 4.9004... ppl: 37.6880  Val Loss: 3.6293\n",
      "Epoch: 2/10... Step: 3840... Loss: 4.3227... ppl: 37.8183  Val Loss: 3.6328\n",
      "Epoch: 2/10... Step: 3872... Loss: 4.0140... ppl: 37.5650  Val Loss: 3.6261\n",
      "Epoch: 2/10... Step: 3904... Loss: 2.9510... ppl: 37.3595  Val Loss: 3.6206\n",
      "Epoch: 2/10... Step: 3936... Loss: 3.8015... ppl: 37.4906  Val Loss: 3.6241\n",
      "Epoch: 2/10... Step: 3968... Loss: 3.9609... ppl: 37.1407  Val Loss: 3.6147\n",
      "Epoch: 2/10... Step: 4000... Loss: 3.7137... ppl: 37.1362  Val Loss: 3.6146\n",
      "Epoch: 2/10... Step: 4032... Loss: 4.1097... ppl: 37.0872  Val Loss: 3.6133\n",
      "Epoch: 2/10... Step: 4064... Loss: 3.6766... ppl: 37.0158  Val Loss: 3.6113\n",
      "Epoch: 2/10... Step: 4096... Loss: 3.6448... ppl: 37.0742  Val Loss: 3.6129\n",
      "Epoch: 2/10... Step: 4128... Loss: 2.9426... ppl: 37.0999  Val Loss: 3.6136\n",
      "Epoch: 2/10... Step: 4160... Loss: 2.8348... ppl: 37.1029  Val Loss: 3.6137\n",
      "Epoch: 2/10... Step: 4192... Loss: 3.2955... ppl: 36.8559  Val Loss: 3.6070\n",
      "Epoch: 2/10... Step: 4224... Loss: 3.3519... ppl: 36.5303  Val Loss: 3.5981\n",
      "Epoch: 2/10... Step: 4256... Loss: 3.4352... ppl: 36.5794  Val Loss: 3.5995\n",
      "Epoch: 2/10... Step: 4288... Loss: 3.4649... ppl: 36.3593  Val Loss: 3.5935\n",
      "Epoch: 2/10... Step: 4320... Loss: 2.8708... ppl: 36.4021  Val Loss: 3.5946\n",
      "Epoch: 2/10... Step: 4352... Loss: 3.0006... ppl: 36.3610  Val Loss: 3.5935\n",
      "Epoch: 2/10... Step: 4384... Loss: 3.1447... ppl: 36.1943  Val Loss: 3.5889\n",
      "Epoch: 2/10... Step: 4416... Loss: 3.9819... ppl: 36.3336  Val Loss: 3.5927\n",
      "Epoch: 2/10... Step: 4448... Loss: 3.0202... ppl: 36.3022  Val Loss: 3.5919\n",
      "Epoch: 2/10... Step: 4480... Loss: 4.3120... ppl: 36.2725  Val Loss: 3.5911\n",
      "Epoch: 2/10... Step: 4512... Loss: 3.2919... ppl: 36.0967  Val Loss: 3.5862\n",
      "Epoch: 2/10... Step: 4544... Loss: 3.5098... ppl: 36.0310  Val Loss: 3.5844\n",
      "Epoch: 2/10... Step: 4576... Loss: 2.7658... ppl: 36.1110  Val Loss: 3.5866\n",
      "Epoch: 2/10... Step: 4608... Loss: 2.9034... ppl: 36.0400  Val Loss: 3.5846\n",
      "Epoch: 2/10... Step: 4640... Loss: 3.6898... ppl: 35.8402  Val Loss: 3.5791\n",
      "Epoch: 2/10... Step: 4672... Loss: 3.5578... ppl: 35.7890  Val Loss: 3.5776\n",
      "Epoch: 3/10... Step: 4704... Loss: 3.0989... ppl: 35.8649  Val Loss: 3.5798\n",
      "Epoch: 3/10... Step: 4736... Loss: 4.1631... ppl: 36.1836  Val Loss: 3.5886\n",
      "Epoch: 3/10... Step: 4768... Loss: 3.1330... ppl: 36.0093  Val Loss: 3.5838\n",
      "Epoch: 3/10... Step: 4800... Loss: 2.5870... ppl: 36.0713  Val Loss: 3.5855\n",
      "Epoch: 3/10... Step: 4832... Loss: 3.3509... ppl: 35.9046  Val Loss: 3.5809\n",
      "Epoch: 3/10... Step: 4864... Loss: 3.7949... ppl: 35.8659  Val Loss: 3.5798\n",
      "Epoch: 3/10... Step: 4896... Loss: 2.4679... ppl: 35.8381  Val Loss: 3.5790\n",
      "Epoch: 3/10... Step: 4928... Loss: 2.0618... ppl: 35.8276  Val Loss: 3.5787\n",
      "Epoch: 3/10... Step: 4960... Loss: 3.2731... ppl: 35.9106  Val Loss: 3.5810\n",
      "Epoch: 3/10... Step: 4992... Loss: 3.3560... ppl: 36.2172  Val Loss: 3.5895\n",
      "Epoch: 3/10... Step: 5024... Loss: 3.5317... ppl: 35.8636  Val Loss: 3.5797\n",
      "Epoch: 3/10... Step: 5056... Loss: 3.0308... ppl: 35.6260  Val Loss: 3.5731\n",
      "Epoch: 3/10... Step: 5088... Loss: 2.7979... ppl: 35.6494  Val Loss: 3.5737\n",
      "Epoch: 3/10... Step: 5120... Loss: 3.7026... ppl: 35.7223  Val Loss: 3.5758\n",
      "Epoch: 3/10... Step: 5152... Loss: 2.9462... ppl: 35.7576  Val Loss: 3.5768\n",
      "Epoch: 3/10... Step: 5184... Loss: 2.8381... ppl: 35.6658  Val Loss: 3.5742\n",
      "Epoch: 3/10... Step: 5216... Loss: 2.9664... ppl: 35.3470  Val Loss: 3.5652\n",
      "Epoch: 3/10... Step: 5248... Loss: 3.0149... ppl: 35.3455  Val Loss: 3.5652\n",
      "Epoch: 3/10... Step: 5280... Loss: 3.1996... ppl: 35.5257  Val Loss: 3.5703\n",
      "Epoch: 3/10... Step: 5312... Loss: 3.4771... ppl: 35.3150  Val Loss: 3.5643\n",
      "Epoch: 3/10... Step: 5344... Loss: 3.6517... ppl: 35.2984  Val Loss: 3.5638\n",
      "Epoch: 3/10... Step: 5376... Loss: 4.0516... ppl: 35.2902  Val Loss: 3.5636\n",
      "Epoch: 3/10... Step: 5408... Loss: 3.7933... ppl: 35.3113  Val Loss: 3.5642\n",
      "Epoch: 3/10... Step: 5440... Loss: 4.3367... ppl: 35.2383  Val Loss: 3.5621\n",
      "Epoch: 3/10... Step: 5472... Loss: 3.7887... ppl: 34.9303  Val Loss: 3.5534\n",
      "Epoch: 3/10... Step: 5504... Loss: 3.7228... ppl: 35.1509  Val Loss: 3.5596\n",
      "Epoch: 3/10... Step: 5536... Loss: 3.2916... ppl: 35.3146  Val Loss: 3.5643\n",
      "Epoch: 3/10... Step: 5568... Loss: 3.0742... ppl: 35.1413  Val Loss: 3.5594\n",
      "Epoch: 3/10... Step: 5600... Loss: 3.3997... ppl: 35.1398  Val Loss: 3.5593\n",
      "Epoch: 3/10... Step: 5632... Loss: 3.3811... ppl: 35.0568  Val Loss: 3.5570\n",
      "Epoch: 3/10... Step: 5664... Loss: 3.1462... ppl: 35.0787  Val Loss: 3.5576\n",
      "Epoch: 3/10... Step: 5696... Loss: 3.6880... ppl: 34.8348  Val Loss: 3.5506\n",
      "Epoch: 3/10... Step: 5728... Loss: 3.2467... ppl: 35.2529  Val Loss: 3.5625\n",
      "Epoch: 3/10... Step: 5760... Loss: 3.8267... ppl: 34.8859  Val Loss: 3.5521\n",
      "Epoch: 3/10... Step: 5792... Loss: 4.0372... ppl: 34.7666  Val Loss: 3.5487\n",
      "Epoch: 3/10... Step: 5824... Loss: 3.2064... ppl: 34.8586  Val Loss: 3.5513\n",
      "Epoch: 3/10... Step: 5856... Loss: 3.4420... ppl: 34.6283  Val Loss: 3.5447\n",
      "Epoch: 3/10... Step: 5888... Loss: 3.1328... ppl: 34.6669  Val Loss: 3.5458\n",
      "Epoch: 3/10... Step: 5920... Loss: 3.5180... ppl: 34.5867  Val Loss: 3.5435\n",
      "Epoch: 3/10... Step: 5952... Loss: 3.7588... ppl: 34.5254  Val Loss: 3.5417\n",
      "Epoch: 3/10... Step: 5984... Loss: 3.2645... ppl: 34.5768  Val Loss: 3.5432\n",
      "Epoch: 3/10... Step: 6016... Loss: 3.8466... ppl: 34.4976  Val Loss: 3.5409\n",
      "Epoch: 3/10... Step: 6048... Loss: 3.4121... ppl: 34.6956  Val Loss: 3.5466\n",
      "Epoch: 3/10... Step: 6080... Loss: 2.7093... ppl: 34.4173  Val Loss: 3.5386\n",
      "Epoch: 3/10... Step: 6112... Loss: 3.2969... ppl: 34.3889  Val Loss: 3.5377\n",
      "Epoch: 3/10... Step: 6144... Loss: 3.0538... ppl: 34.3266  Val Loss: 3.5359\n",
      "Epoch: 3/10... Step: 6176... Loss: 3.2578... ppl: 34.4498  Val Loss: 3.5395\n",
      "Epoch: 3/10... Step: 6208... Loss: 3.4903... ppl: 34.3540  Val Loss: 3.5367\n",
      "Epoch: 3/10... Step: 6240... Loss: 2.7225... ppl: 34.3035  Val Loss: 3.5352\n",
      "Epoch: 3/10... Step: 6272... Loss: 2.7643... ppl: 34.5292  Val Loss: 3.5418\n",
      "Epoch: 3/10... Step: 6304... Loss: 3.0944... ppl: 34.2100  Val Loss: 3.5325\n",
      "Epoch: 3/10... Step: 6336... Loss: 3.4055... ppl: 34.1440  Val Loss: 3.5306\n",
      "Epoch: 3/10... Step: 6368... Loss: 3.0478... ppl: 34.1158  Val Loss: 3.5298\n",
      "Epoch: 3/10... Step: 6400... Loss: 3.7823... ppl: 34.1240  Val Loss: 3.5300\n",
      "Epoch: 3/10... Step: 6432... Loss: 3.5291... ppl: 34.1078  Val Loss: 3.5295\n",
      "Epoch: 3/10... Step: 6464... Loss: 3.5216... ppl: 34.3156  Val Loss: 3.5356\n",
      "Epoch: 3/10... Step: 6496... Loss: 2.8661... ppl: 34.1544  Val Loss: 3.5309\n",
      "Epoch: 3/10... Step: 6528... Loss: 3.0895... ppl: 34.1121  Val Loss: 3.5297\n",
      "Epoch: 3/10... Step: 6560... Loss: 3.3494... ppl: 33.9565  Val Loss: 3.5251\n",
      "Epoch: 3/10... Step: 6592... Loss: 2.5482... ppl: 34.0101  Val Loss: 3.5267\n",
      "Epoch: 3/10... Step: 6624... Loss: 3.6018... ppl: 33.7467  Val Loss: 3.5189\n",
      "Epoch: 3/10... Step: 6656... Loss: 3.0208... ppl: 33.7899  Val Loss: 3.5202\n",
      "Epoch: 3/10... Step: 6688... Loss: 2.8055... ppl: 33.9998  Val Loss: 3.5264\n",
      "Epoch: 3/10... Step: 6720... Loss: 2.5222... ppl: 33.7770  Val Loss: 3.5198\n",
      "Epoch: 3/10... Step: 6752... Loss: 3.6402... ppl: 33.8341  Val Loss: 3.5215\n",
      "Epoch: 3/10... Step: 6784... Loss: 3.2186... ppl: 33.9099  Val Loss: 3.5237\n",
      "Epoch: 3/10... Step: 6816... Loss: 2.7826... ppl: 33.8692  Val Loss: 3.5225\n",
      "Epoch: 3/10... Step: 6848... Loss: 2.8898... ppl: 33.6709  Val Loss: 3.5166\n",
      "Epoch: 3/10... Step: 6880... Loss: 2.9773... ppl: 33.7250  Val Loss: 3.5182\n",
      "Epoch: 3/10... Step: 6912... Loss: 4.1261... ppl: 33.8382  Val Loss: 3.5216\n",
      "Epoch: 3/10... Step: 6944... Loss: 3.1671... ppl: 34.0872  Val Loss: 3.5289\n",
      "Epoch: 3/10... Step: 6976... Loss: 3.3784... ppl: 33.7444  Val Loss: 3.5188\n",
      "Epoch: 3/10... Step: 7008... Loss: 3.4352... ppl: 33.5969  Val Loss: 3.5144\n",
      "Epoch: 4/10... Step: 7040... Loss: 3.5713... ppl: 33.5556  Val Loss: 3.5132\n",
      "Epoch: 4/10... Step: 7072... Loss: 2.9333... ppl: 34.0808  Val Loss: 3.5287\n",
      "Epoch: 4/10... Step: 7104... Loss: 3.3270... ppl: 33.7509  Val Loss: 3.5190\n",
      "Epoch: 4/10... Step: 7136... Loss: 2.8193... ppl: 33.8922  Val Loss: 3.5232\n",
      "Epoch: 4/10... Step: 7168... Loss: 3.3544... ppl: 33.8916  Val Loss: 3.5232\n",
      "Epoch: 4/10... Step: 7200... Loss: 2.7426... ppl: 33.7705  Val Loss: 3.5196\n",
      "Epoch: 4/10... Step: 7232... Loss: 2.9769... ppl: 33.8310  Val Loss: 3.5214\n",
      "Epoch: 4/10... Step: 7264... Loss: 4.1893... ppl: 33.7706  Val Loss: 3.5196\n",
      "Epoch: 4/10... Step: 7296... Loss: 3.5247... ppl: 33.9221  Val Loss: 3.5241\n",
      "Epoch: 4/10... Step: 7328... Loss: 2.6650... ppl: 34.3131  Val Loss: 3.5355\n",
      "Epoch: 4/10... Step: 7360... Loss: 2.8392... ppl: 34.1804  Val Loss: 3.5317\n",
      "Epoch: 4/10... Step: 7392... Loss: 3.1114... ppl: 33.8609  Val Loss: 3.5223\n",
      "Epoch: 4/10... Step: 7424... Loss: 2.7451... ppl: 33.7696  Val Loss: 3.5196\n",
      "Epoch: 4/10... Step: 7456... Loss: 2.7416... ppl: 33.9521  Val Loss: 3.5250\n",
      "Epoch: 4/10... Step: 7488... Loss: 3.5199... ppl: 33.9824  Val Loss: 3.5258\n",
      "Epoch: 4/10... Step: 7520... Loss: 1.9836... ppl: 34.0841  Val Loss: 3.5288\n",
      "Epoch: 4/10... Step: 7552... Loss: 3.1645... ppl: 33.8502  Val Loss: 3.5219\n",
      "Epoch: 4/10... Step: 7584... Loss: 2.5578... ppl: 33.8637  Val Loss: 3.5223\n",
      "Epoch: 4/10... Step: 7616... Loss: 2.9544... ppl: 33.9176  Val Loss: 3.5239\n",
      "Epoch: 4/10... Step: 7648... Loss: 3.4078... ppl: 33.8631  Val Loss: 3.5223\n",
      "Epoch: 4/10... Step: 7680... Loss: 2.8274... ppl: 33.7964  Val Loss: 3.5204\n",
      "Epoch: 4/10... Step: 7712... Loss: 2.7948... ppl: 33.8640  Val Loss: 3.5224\n",
      "Epoch: 4/10... Step: 7744... Loss: 3.3297... ppl: 33.8298  Val Loss: 3.5213\n",
      "Epoch: 4/10... Step: 7776... Loss: 3.7756... ppl: 33.8966  Val Loss: 3.5233\n",
      "Epoch: 4/10... Step: 7808... Loss: 3.4958... ppl: 33.5497  Val Loss: 3.5130\n",
      "Epoch: 4/10... Step: 7840... Loss: 2.1599... ppl: 33.7438  Val Loss: 3.5188\n",
      "Epoch: 4/10... Step: 7872... Loss: 3.7570... ppl: 33.9061  Val Loss: 3.5236\n",
      "Epoch: 4/10... Step: 7904... Loss: 2.2388... ppl: 33.8168  Val Loss: 3.5210\n",
      "Epoch: 4/10... Step: 7936... Loss: 3.0534... ppl: 33.7071  Val Loss: 3.5177\n",
      "Epoch: 4/10... Step: 7968... Loss: 3.8568... ppl: 33.6362  Val Loss: 3.5156\n",
      "Epoch: 4/10... Step: 8000... Loss: 3.0089... ppl: 33.7753  Val Loss: 3.5197\n",
      "Epoch: 4/10... Step: 8032... Loss: 3.3838... ppl: 33.7556  Val Loss: 3.5191\n",
      "Epoch: 4/10... Step: 8064... Loss: 2.4293... ppl: 33.8979  Val Loss: 3.5234\n",
      "Epoch: 4/10... Step: 8096... Loss: 3.0555... ppl: 33.7803  Val Loss: 3.5199\n",
      "Epoch: 4/10... Step: 8128... Loss: 2.3833... ppl: 33.5898  Val Loss: 3.5142\n",
      "Epoch: 4/10... Step: 8160... Loss: 2.9873... ppl: 33.7269  Val Loss: 3.5183\n",
      "Epoch: 4/10... Step: 8192... Loss: 3.3985... ppl: 33.6634  Val Loss: 3.5164\n",
      "Epoch: 4/10... Step: 8224... Loss: 2.9607... ppl: 33.5017  Val Loss: 3.5116\n",
      "Epoch: 4/10... Step: 8256... Loss: 2.7310... ppl: 33.5405  Val Loss: 3.5128\n",
      "Epoch: 4/10... Step: 8288... Loss: 3.0696... ppl: 33.5233  Val Loss: 3.5122\n",
      "Epoch: 4/10... Step: 8320... Loss: 3.2034... ppl: 33.4704  Val Loss: 3.5107\n",
      "Epoch: 4/10... Step: 8352... Loss: 2.7893... ppl: 33.4010  Val Loss: 3.5086\n",
      "Epoch: 4/10... Step: 8384... Loss: 2.6167... ppl: 33.6308  Val Loss: 3.5154\n",
      "Epoch: 4/10... Step: 8416... Loss: 2.7273... ppl: 33.4515  Val Loss: 3.5101\n",
      "Epoch: 4/10... Step: 8448... Loss: 4.1374... ppl: 33.4323  Val Loss: 3.5095\n",
      "Epoch: 4/10... Step: 8480... Loss: 2.7617... ppl: 33.5068  Val Loss: 3.5117\n",
      "Epoch: 4/10... Step: 8512... Loss: 2.4311... ppl: 33.4029  Val Loss: 3.5086\n",
      "Epoch: 4/10... Step: 8544... Loss: 3.1502... ppl: 33.4646  Val Loss: 3.5105\n",
      "Epoch: 4/10... Step: 8576... Loss: 2.9970... ppl: 33.4673  Val Loss: 3.5106\n",
      "Epoch: 4/10... Step: 8608... Loss: 3.4396... ppl: 33.5413  Val Loss: 3.5128\n",
      "Epoch: 4/10... Step: 8640... Loss: 3.2981... ppl: 33.5174  Val Loss: 3.5121\n",
      "Epoch: 4/10... Step: 8672... Loss: 2.6861... ppl: 33.3771  Val Loss: 3.5079\n",
      "Epoch: 4/10... Step: 8704... Loss: 3.6545... ppl: 33.3045  Val Loss: 3.5057\n",
      "Epoch: 4/10... Step: 8736... Loss: 3.0557... ppl: 33.3672  Val Loss: 3.5076\n",
      "Epoch: 4/10... Step: 8768... Loss: 2.6691... ppl: 33.3912  Val Loss: 3.5083\n",
      "Epoch: 4/10... Step: 8800... Loss: 3.4796... ppl: 33.5065  Val Loss: 3.5117\n",
      "Epoch: 4/10... Step: 8832... Loss: 3.0190... ppl: 33.3849  Val Loss: 3.5081\n",
      "Epoch: 4/10... Step: 8864... Loss: 3.2581... ppl: 33.5797  Val Loss: 3.5139\n",
      "Epoch: 4/10... Step: 8896... Loss: 2.6888... ppl: 33.3605  Val Loss: 3.5074\n",
      "Epoch: 4/10... Step: 8928... Loss: 2.8660... ppl: 33.4107  Val Loss: 3.5089\n",
      "Epoch: 4/10... Step: 8960... Loss: 2.3570... ppl: 33.0941  Val Loss: 3.4994\n",
      "Epoch: 4/10... Step: 8992... Loss: 2.9709... ppl: 33.0939  Val Loss: 3.4993\n",
      "Epoch: 4/10... Step: 9024... Loss: 3.4680... ppl: 33.3386  Val Loss: 3.5067\n",
      "Epoch: 4/10... Step: 9056... Loss: 2.9415... ppl: 33.1878  Val Loss: 3.5022\n",
      "Epoch: 4/10... Step: 9088... Loss: 3.3729... ppl: 33.1426  Val Loss: 3.5008\n",
      "Epoch: 4/10... Step: 9120... Loss: 2.6440... ppl: 33.3582  Val Loss: 3.5073\n",
      "Epoch: 4/10... Step: 9152... Loss: 2.9281... ppl: 33.3946  Val Loss: 3.5084\n",
      "Epoch: 4/10... Step: 9184... Loss: 3.2995... ppl: 33.1302  Val Loss: 3.5004\n",
      "Epoch: 4/10... Step: 9216... Loss: 2.3810... ppl: 33.1602  Val Loss: 3.5013\n",
      "Epoch: 4/10... Step: 9248... Loss: 3.6082... ppl: 33.1902  Val Loss: 3.5023\n",
      "Epoch: 4/10... Step: 9280... Loss: 2.6784... ppl: 33.7309  Val Loss: 3.5184\n",
      "Epoch: 4/10... Step: 9312... Loss: 3.4379... ppl: 33.3445  Val Loss: 3.5069\n",
      "Epoch: 4/10... Step: 9344... Loss: 3.2106... ppl: 33.1034  Val Loss: 3.4996\n",
      "Epoch: 5/10... Step: 9376... Loss: 1.9162... ppl: 33.1329  Val Loss: 3.5005\n",
      "Epoch: 5/10... Step: 9408... Loss: 3.4178... ppl: 33.5735  Val Loss: 3.5137\n",
      "Epoch: 5/10... Step: 9440... Loss: 2.8730... ppl: 33.1945  Val Loss: 3.5024\n",
      "Epoch: 5/10... Step: 9472... Loss: 2.7464... ppl: 33.3312  Val Loss: 3.5065\n",
      "Epoch: 5/10... Step: 9504... Loss: 3.0820... ppl: 33.4377  Val Loss: 3.5097\n",
      "Epoch: 5/10... Step: 9536... Loss: 2.2002... ppl: 33.3301  Val Loss: 3.5065\n",
      "Epoch: 5/10... Step: 9568... Loss: 1.8938... ppl: 33.5263  Val Loss: 3.5123\n",
      "Epoch: 5/10... Step: 9600... Loss: 3.0380... ppl: 33.3564  Val Loss: 3.5073\n",
      "Epoch: 5/10... Step: 9632... Loss: 2.9473... ppl: 33.5344  Val Loss: 3.5126\n",
      "Epoch: 5/10... Step: 9664... Loss: 3.1378... ppl: 33.7527  Val Loss: 3.5191\n",
      "Epoch: 5/10... Step: 9696... Loss: 2.2780... ppl: 33.9669  Val Loss: 3.5254\n",
      "Epoch: 5/10... Step: 9728... Loss: 3.2436... ppl: 33.7018  Val Loss: 3.5176\n",
      "Epoch: 5/10... Step: 9760... Loss: 2.4705... ppl: 33.4386  Val Loss: 3.5097\n",
      "Epoch: 5/10... Step: 9792... Loss: 3.3466... ppl: 33.5645  Val Loss: 3.5135\n",
      "Epoch: 5/10... Step: 9824... Loss: 3.2465... ppl: 33.6611  Val Loss: 3.5163\n",
      "Epoch: 5/10... Step: 9856... Loss: 2.0560... ppl: 33.9242  Val Loss: 3.5241\n",
      "Epoch: 5/10... Step: 9888... Loss: 3.2088... ppl: 33.7119  Val Loss: 3.5178\n",
      "Epoch: 5/10... Step: 9920... Loss: 2.9095... ppl: 33.7045  Val Loss: 3.5176\n",
      "Epoch: 5/10... Step: 9952... Loss: 3.2603... ppl: 33.5825  Val Loss: 3.5140\n",
      "Epoch: 5/10... Step: 9984... Loss: 3.0885... ppl: 33.7304  Val Loss: 3.5184\n",
      "Epoch: 5/10... Step: 10016... Loss: 2.7561... ppl: 33.7171  Val Loss: 3.5180\n",
      "Epoch: 5/10... Step: 10048... Loss: 2.7580... ppl: 33.7597  Val Loss: 3.5193\n",
      "Epoch: 5/10... Step: 10080... Loss: 2.8907... ppl: 33.6968  Val Loss: 3.5174\n",
      "Epoch: 5/10... Step: 10112... Loss: 3.7815... ppl: 33.7382  Val Loss: 3.5186\n",
      "Epoch: 5/10... Step: 10144... Loss: 2.7484... ppl: 33.4750  Val Loss: 3.5108\n",
      "Epoch: 5/10... Step: 10176... Loss: 2.5139... ppl: 33.5210  Val Loss: 3.5122\n",
      "Epoch: 5/10... Step: 10208... Loss: 3.2178... ppl: 33.6957  Val Loss: 3.5174\n",
      "Epoch: 5/10... Step: 10240... Loss: 2.9759... ppl: 33.7047  Val Loss: 3.5176\n",
      "Epoch: 5/10... Step: 10272... Loss: 3.6103... ppl: 33.6830  Val Loss: 3.5170\n",
      "Epoch: 5/10... Step: 10304... Loss: 3.6876... ppl: 33.5617  Val Loss: 3.5134\n",
      "Epoch: 5/10... Step: 10336... Loss: 2.8859... ppl: 33.6513  Val Loss: 3.5161\n",
      "Epoch: 5/10... Step: 10368... Loss: 2.8736... ppl: 33.7025  Val Loss: 3.5176\n",
      "Epoch: 5/10... Step: 10400... Loss: 2.9909... ppl: 33.6697  Val Loss: 3.5166\n",
      "Epoch: 5/10... Step: 10432... Loss: 2.9771... ppl: 33.8541  Val Loss: 3.5221\n",
      "Epoch: 5/10... Step: 10464... Loss: 3.0655... ppl: 33.5977  Val Loss: 3.5145\n",
      "Epoch: 5/10... Step: 10496... Loss: 2.5570... ppl: 33.6279  Val Loss: 3.5154\n",
      "Epoch: 5/10... Step: 10528... Loss: 3.1764... ppl: 33.7762  Val Loss: 3.5198\n",
      "Epoch: 5/10... Step: 10560... Loss: 3.4544... ppl: 33.4476  Val Loss: 3.5100\n",
      "Epoch: 5/10... Step: 10592... Loss: 2.9784... ppl: 33.5070  Val Loss: 3.5118\n",
      "Epoch: 5/10... Step: 10624... Loss: 2.9266... ppl: 33.5240  Val Loss: 3.5123\n",
      "Epoch: 5/10... Step: 10656... Loss: 3.0366... ppl: 33.5042  Val Loss: 3.5117\n",
      "Epoch: 5/10... Step: 10688... Loss: 2.7836... ppl: 33.4287  Val Loss: 3.5094\n",
      "Epoch: 5/10... Step: 10720... Loss: 2.8069... ppl: 33.6500  Val Loss: 3.5160\n",
      "Epoch: 5/10... Step: 10752... Loss: 2.5661... ppl: 33.7010  Val Loss: 3.5175\n",
      "Epoch: 5/10... Step: 10784... Loss: 3.5865... ppl: 33.5352  Val Loss: 3.5126\n",
      "Epoch: 5/10... Step: 10816... Loss: 2.9964... ppl: 33.6617  Val Loss: 3.5164\n",
      "Epoch: 5/10... Step: 10848... Loss: 3.3729... ppl: 33.4894  Val Loss: 3.5112\n",
      "Epoch: 5/10... Step: 10880... Loss: 2.8082... ppl: 33.5812  Val Loss: 3.5140\n",
      "Epoch: 5/10... Step: 10912... Loss: 2.3811... ppl: 33.5050  Val Loss: 3.5117\n",
      "Epoch: 5/10... Step: 10944... Loss: 3.0120... ppl: 33.5437  Val Loss: 3.5128\n",
      "Epoch: 5/10... Step: 10976... Loss: 3.1666... ppl: 33.6287  Val Loss: 3.5154\n",
      "Epoch: 5/10... Step: 11008... Loss: 3.0697... ppl: 33.4212  Val Loss: 3.5092\n",
      "Epoch: 5/10... Step: 11040... Loss: 3.5485... ppl: 33.4848  Val Loss: 3.5111\n",
      "Epoch: 5/10... Step: 11072... Loss: 3.5914... ppl: 33.4835  Val Loss: 3.5111\n",
      "Epoch: 5/10... Step: 11104... Loss: 3.1185... ppl: 33.4737  Val Loss: 3.5108\n",
      "Epoch: 5/10... Step: 11136... Loss: 3.7245... ppl: 33.4643  Val Loss: 3.5105\n",
      "Epoch: 5/10... Step: 11168... Loss: 2.4730... ppl: 33.5284  Val Loss: 3.5124\n",
      "Epoch: 5/10... Step: 11200... Loss: 2.8316... ppl: 33.8759  Val Loss: 3.5227\n",
      "Epoch: 5/10... Step: 11232... Loss: 2.6568... ppl: 33.5379  Val Loss: 3.5127\n",
      "Epoch: 5/10... Step: 11264... Loss: 2.0327... ppl: 33.5503  Val Loss: 3.5130\n",
      "Epoch: 5/10... Step: 11296... Loss: 2.6786... ppl: 33.5245  Val Loss: 3.5123\n",
      "Epoch: 5/10... Step: 11328... Loss: 3.4692... ppl: 33.2551  Val Loss: 3.5042\n",
      "Epoch: 5/10... Step: 11360... Loss: 2.7253... ppl: 33.5893  Val Loss: 3.5142\n",
      "Epoch: 5/10... Step: 11392... Loss: 2.7037... ppl: 33.5020  Val Loss: 3.5116\n",
      "Epoch: 5/10... Step: 11424... Loss: 3.4712... ppl: 33.3617  Val Loss: 3.5074\n",
      "Epoch: 5/10... Step: 11456... Loss: 2.2493... ppl: 33.5752  Val Loss: 3.5138\n",
      "Epoch: 5/10... Step: 11488... Loss: 2.9011... ppl: 33.6517  Val Loss: 3.5161\n",
      "Epoch: 5/10... Step: 11520... Loss: 2.3436... ppl: 33.4026  Val Loss: 3.5086\n",
      "Epoch: 5/10... Step: 11552... Loss: 3.6916... ppl: 33.3510  Val Loss: 3.5071\n",
      "Epoch: 5/10... Step: 11584... Loss: 2.8848... ppl: 33.3382  Val Loss: 3.5067\n",
      "Epoch: 5/10... Step: 11616... Loss: 2.4042... ppl: 33.7965  Val Loss: 3.5204\n",
      "Epoch: 5/10... Step: 11648... Loss: 2.8129... ppl: 33.6232  Val Loss: 3.5152\n",
      "Epoch: 5/10... Step: 11680... Loss: 3.1989... ppl: 33.3563  Val Loss: 3.5072\n",
      "Epoch: 5/10... Step: 11712... Loss: 2.9945... ppl: 33.3927  Val Loss: 3.5083\n",
      "Epoch: 6/10... Step: 11744... Loss: 3.4958... ppl: 33.6933  Val Loss: 3.5173\n",
      "Epoch: 6/10... Step: 11776... Loss: 2.8260... ppl: 33.5896  Val Loss: 3.5142\n",
      "Epoch: 6/10... Step: 11808... Loss: 2.6316... ppl: 33.6072  Val Loss: 3.5147\n",
      "Epoch: 6/10... Step: 11840... Loss: 3.3783... ppl: 33.6409  Val Loss: 3.5157\n",
      "Epoch: 6/10... Step: 11872... Loss: 2.8683... ppl: 33.5526  Val Loss: 3.5131\n",
      "Epoch: 6/10... Step: 11904... Loss: 2.9689... ppl: 33.6990  Val Loss: 3.5175\n",
      "Epoch: 6/10... Step: 11936... Loss: 2.8572... ppl: 33.6571  Val Loss: 3.5162\n",
      "Epoch: 6/10... Step: 11968... Loss: 2.7119... ppl: 33.7372  Val Loss: 3.5186\n",
      "Epoch: 6/10... Step: 12000... Loss: 2.5413... ppl: 33.8535  Val Loss: 3.5220\n",
      "Epoch: 6/10... Step: 12032... Loss: 2.9511... ppl: 34.3146  Val Loss: 3.5356\n",
      "Epoch: 6/10... Step: 12064... Loss: 3.0762... ppl: 34.0318  Val Loss: 3.5273\n",
      "Epoch: 6/10... Step: 12096... Loss: 3.7101... ppl: 33.8203  Val Loss: 3.5211\n",
      "Epoch: 6/10... Step: 12128... Loss: 2.5237... ppl: 33.8597  Val Loss: 3.5222\n",
      "Epoch: 6/10... Step: 12160... Loss: 2.9045... ppl: 34.0561  Val Loss: 3.5280\n",
      "Epoch: 6/10... Step: 12192... Loss: 2.8773... ppl: 34.3234  Val Loss: 3.5358\n",
      "Epoch: 6/10... Step: 12224... Loss: 2.5949... ppl: 34.1413  Val Loss: 3.5305\n",
      "Epoch: 6/10... Step: 12256... Loss: 2.7022... ppl: 33.9676  Val Loss: 3.5254\n",
      "Epoch: 6/10... Step: 12288... Loss: 2.7905... ppl: 33.8539  Val Loss: 3.5221\n",
      "Epoch: 6/10... Step: 12320... Loss: 2.8357... ppl: 34.0893  Val Loss: 3.5290\n",
      "Epoch: 6/10... Step: 12352... Loss: 2.6194... ppl: 34.1535  Val Loss: 3.5309\n",
      "Epoch: 6/10... Step: 12384... Loss: 2.9417... ppl: 34.0567  Val Loss: 3.5280\n",
      "Epoch: 6/10... Step: 12416... Loss: 3.0773... ppl: 34.1464  Val Loss: 3.5307\n",
      "Epoch: 6/10... Step: 12448... Loss: 2.9641... ppl: 34.1534  Val Loss: 3.5309\n",
      "Epoch: 6/10... Step: 12480... Loss: 2.6528... ppl: 34.0866  Val Loss: 3.5289\n",
      "Epoch: 6/10... Step: 12512... Loss: 2.7340... ppl: 33.9306  Val Loss: 3.5243\n",
      "Epoch: 6/10... Step: 12544... Loss: 2.8573... ppl: 34.0868  Val Loss: 3.5289\n",
      "Epoch: 6/10... Step: 12576... Loss: 2.4634... ppl: 34.3317  Val Loss: 3.5361\n",
      "Epoch: 6/10... Step: 12608... Loss: 3.1279... ppl: 34.1961  Val Loss: 3.5321\n",
      "Epoch: 6/10... Step: 12640... Loss: 2.6246... ppl: 33.9891  Val Loss: 3.5260\n",
      "Epoch: 6/10... Step: 12672... Loss: 3.1451... ppl: 34.0751  Val Loss: 3.5286\n",
      "Epoch: 6/10... Step: 12704... Loss: 2.4441... ppl: 34.1070  Val Loss: 3.5295\n",
      "Epoch: 6/10... Step: 12736... Loss: 3.2191... ppl: 33.9988  Val Loss: 3.5263\n",
      "Epoch: 6/10... Step: 12768... Loss: 2.5960... ppl: 34.3366  Val Loss: 3.5362\n",
      "Epoch: 6/10... Step: 12800... Loss: 2.9792... ppl: 34.0273  Val Loss: 3.5272\n",
      "Epoch: 6/10... Step: 12832... Loss: 2.9629... ppl: 34.0322  Val Loss: 3.5273\n",
      "Epoch: 6/10... Step: 12864... Loss: 2.9516... ppl: 34.2192  Val Loss: 3.5328\n",
      "Epoch: 6/10... Step: 12896... Loss: 3.1737... ppl: 33.9279  Val Loss: 3.5242\n",
      "Epoch: 6/10... Step: 12928... Loss: 2.7513... ppl: 33.9375  Val Loss: 3.5245\n",
      "Epoch: 6/10... Step: 12960... Loss: 2.9603... ppl: 33.9728  Val Loss: 3.5256\n",
      "Epoch: 6/10... Step: 12992... Loss: 3.0565... ppl: 34.0033  Val Loss: 3.5265\n",
      "Epoch: 6/10... Step: 13024... Loss: 2.3860... ppl: 34.0312  Val Loss: 3.5273\n",
      "Epoch: 6/10... Step: 13056... Loss: 2.6895... ppl: 34.0472  Val Loss: 3.5277\n",
      "Epoch: 6/10... Step: 13088... Loss: 2.6397... ppl: 34.3265  Val Loss: 3.5359\n",
      "Epoch: 6/10... Step: 13120... Loss: 3.2858... ppl: 34.1835  Val Loss: 3.5317\n",
      "Epoch: 6/10... Step: 13152... Loss: 2.8254... ppl: 34.2633  Val Loss: 3.5341\n",
      "Epoch: 6/10... Step: 13184... Loss: 3.3203... ppl: 34.0794  Val Loss: 3.5287\n",
      "Epoch: 6/10... Step: 13216... Loss: 3.3345... ppl: 34.0401  Val Loss: 3.5275\n",
      "Epoch: 6/10... Step: 13248... Loss: 3.2783... ppl: 34.0855  Val Loss: 3.5289\n",
      "Epoch: 6/10... Step: 13280... Loss: 3.1660... ppl: 34.0760  Val Loss: 3.5286\n",
      "Epoch: 6/10... Step: 13312... Loss: 2.8980... ppl: 34.1864  Val Loss: 3.5318\n",
      "Epoch: 6/10... Step: 13344... Loss: 2.5147... ppl: 34.1185  Val Loss: 3.5298\n",
      "Epoch: 6/10... Step: 13376... Loss: 2.6604... ppl: 34.1796  Val Loss: 3.5316\n",
      "Epoch: 6/10... Step: 13408... Loss: 2.2261... ppl: 34.1598  Val Loss: 3.5310\n",
      "Epoch: 6/10... Step: 13440... Loss: 3.2780... ppl: 34.1610  Val Loss: 3.5311\n",
      "Epoch: 6/10... Step: 13472... Loss: 3.1353... ppl: 34.1081  Val Loss: 3.5295\n",
      "Epoch: 6/10... Step: 13504... Loss: 2.9300... ppl: 34.1996  Val Loss: 3.5322\n",
      "Epoch: 6/10... Step: 13536... Loss: 3.0922... ppl: 34.4499  Val Loss: 3.5395\n",
      "Epoch: 6/10... Step: 13568... Loss: 2.7353... ppl: 34.1906  Val Loss: 3.5320\n",
      "Epoch: 6/10... Step: 13600... Loss: 2.7769... ppl: 34.1079  Val Loss: 3.5295\n",
      "Epoch: 6/10... Step: 13632... Loss: 3.7063... ppl: 34.4344  Val Loss: 3.5391\n",
      "Epoch: 6/10... Step: 13664... Loss: 2.6700... ppl: 33.9656  Val Loss: 3.5253\n",
      "Epoch: 6/10... Step: 13696... Loss: 3.1611... ppl: 34.1718  Val Loss: 3.5314\n",
      "Epoch: 6/10... Step: 13728... Loss: 2.4301... ppl: 34.2479  Val Loss: 3.5336\n",
      "Epoch: 6/10... Step: 13760... Loss: 3.2742... ppl: 34.0069  Val Loss: 3.5266\n",
      "Epoch: 6/10... Step: 13792... Loss: 3.0251... ppl: 34.2371  Val Loss: 3.5333\n",
      "Epoch: 6/10... Step: 13824... Loss: 2.4305... ppl: 34.3145  Val Loss: 3.5356\n",
      "Epoch: 6/10... Step: 13856... Loss: 3.2061... ppl: 34.2672  Val Loss: 3.5342\n",
      "Epoch: 6/10... Step: 13888... Loss: 2.7504... ppl: 33.9511  Val Loss: 3.5249\n",
      "Epoch: 6/10... Step: 13920... Loss: 3.3752... ppl: 33.9858  Val Loss: 3.5259\n",
      "Epoch: 6/10... Step: 13952... Loss: 2.1745... ppl: 34.2144  Val Loss: 3.5326\n",
      "Epoch: 6/10... Step: 13984... Loss: 3.0577... ppl: 34.3295  Val Loss: 3.5360\n",
      "Epoch: 6/10... Step: 14016... Loss: 2.5282... ppl: 34.1344  Val Loss: 3.5303\n",
      "Epoch: 6/10... Step: 14048... Loss: 2.4470... ppl: 33.9991  Val Loss: 3.5263\n",
      "Epoch: 7/10... Step: 14080... Loss: 2.9611... ppl: 34.1313  Val Loss: 3.5302\n",
      "Epoch: 7/10... Step: 14112... Loss: 2.0985... ppl: 34.3990  Val Loss: 3.5380\n",
      "Epoch: 7/10... Step: 14144... Loss: 2.7581... ppl: 34.0991  Val Loss: 3.5293\n",
      "Epoch: 7/10... Step: 14176... Loss: 2.5659... ppl: 34.2484  Val Loss: 3.5336\n",
      "Epoch: 7/10... Step: 14208... Loss: 2.4533... ppl: 34.1704  Val Loss: 3.5314\n",
      "Epoch: 7/10... Step: 14240... Loss: 2.6467... ppl: 34.2333  Val Loss: 3.5332\n",
      "Epoch: 7/10... Step: 14272... Loss: 2.4246... ppl: 34.3925  Val Loss: 3.5378\n",
      "Epoch: 7/10... Step: 14304... Loss: 2.2771... ppl: 34.2772  Val Loss: 3.5345\n",
      "Epoch: 7/10... Step: 14336... Loss: 2.8123... ppl: 34.4182  Val Loss: 3.5386\n",
      "Epoch: 7/10... Step: 14368... Loss: 3.4141... ppl: 34.9981  Val Loss: 3.5553\n",
      "Epoch: 7/10... Step: 14400... Loss: 2.6758... ppl: 34.7339  Val Loss: 3.5477\n",
      "Epoch: 7/10... Step: 14432... Loss: 2.9291... ppl: 34.4779  Val Loss: 3.5403\n",
      "Epoch: 7/10... Step: 14464... Loss: 3.0102... ppl: 34.4788  Val Loss: 3.5403\n",
      "Epoch: 7/10... Step: 14496... Loss: 3.0963... ppl: 34.7484  Val Loss: 3.5481\n",
      "Epoch: 7/10... Step: 14528... Loss: 2.1304... ppl: 34.9409  Val Loss: 3.5537\n",
      "Epoch: 7/10... Step: 14560... Loss: 2.3922... ppl: 34.9235  Val Loss: 3.5532\n",
      "Epoch: 7/10... Step: 14592... Loss: 2.5055... ppl: 34.6710  Val Loss: 3.5459\n",
      "Epoch: 7/10... Step: 14624... Loss: 2.1879... ppl: 34.6351  Val Loss: 3.5449\n",
      "Epoch: 7/10... Step: 14656... Loss: 2.9334... ppl: 34.8294  Val Loss: 3.5505\n",
      "Epoch: 7/10... Step: 14688... Loss: 3.0357... ppl: 34.8693  Val Loss: 3.5516\n",
      "Epoch: 7/10... Step: 14720... Loss: 3.6041... ppl: 34.8510  Val Loss: 3.5511\n",
      "Epoch: 7/10... Step: 14752... Loss: 3.0986... ppl: 34.9919  Val Loss: 3.5551\n",
      "Epoch: 7/10... Step: 14784... Loss: 2.0791... ppl: 34.8686  Val Loss: 3.5516\n",
      "Epoch: 7/10... Step: 14816... Loss: 2.6194... ppl: 34.9706  Val Loss: 3.5545\n",
      "Epoch: 7/10... Step: 14848... Loss: 2.5949... ppl: 34.6742  Val Loss: 3.5460\n",
      "Epoch: 7/10... Step: 14880... Loss: 2.7331... ppl: 34.8583  Val Loss: 3.5513\n",
      "Epoch: 7/10... Step: 14912... Loss: 2.5822... ppl: 35.2099  Val Loss: 3.5613\n",
      "Epoch: 7/10... Step: 14944... Loss: 2.9886... ppl: 34.9224  Val Loss: 3.5531\n",
      "Epoch: 7/10... Step: 14976... Loss: 2.7164... ppl: 34.7897  Val Loss: 3.5493\n",
      "Epoch: 7/10... Step: 15008... Loss: 2.9099... ppl: 34.9091  Val Loss: 3.5527\n",
      "Epoch: 7/10... Step: 15040... Loss: 2.0998... ppl: 34.8397  Val Loss: 3.5508\n",
      "Epoch: 7/10... Step: 15072... Loss: 3.1381... ppl: 34.7779  Val Loss: 3.5490\n",
      "Epoch: 7/10... Step: 15104... Loss: 2.7711... ppl: 35.1653  Val Loss: 3.5601\n",
      "Epoch: 7/10... Step: 15136... Loss: 3.2746... ppl: 34.9502  Val Loss: 3.5539\n",
      "Epoch: 7/10... Step: 15168... Loss: 2.4021... ppl: 34.8657  Val Loss: 3.5515\n",
      "Epoch: 7/10... Step: 15200... Loss: 2.6736... ppl: 35.0156  Val Loss: 3.5558\n",
      "Epoch: 7/10... Step: 15232... Loss: 3.2272... ppl: 34.8215  Val Loss: 3.5502\n",
      "Epoch: 7/10... Step: 15264... Loss: 2.7947... ppl: 34.8064  Val Loss: 3.5498\n",
      "Epoch: 7/10... Step: 15296... Loss: 2.8681... ppl: 34.7704  Val Loss: 3.5488\n",
      "Epoch: 7/10... Step: 15328... Loss: 2.5891... ppl: 34.7149  Val Loss: 3.5472\n",
      "Epoch: 7/10... Step: 15360... Loss: 2.7171... ppl: 34.8561  Val Loss: 3.5512\n",
      "Epoch: 7/10... Step: 15392... Loss: 2.6965... ppl: 34.7737  Val Loss: 3.5489\n",
      "Epoch: 7/10... Step: 15424... Loss: 2.2232... ppl: 35.0941  Val Loss: 3.5580\n",
      "Epoch: 7/10... Step: 15456... Loss: 3.1445... ppl: 35.0282  Val Loss: 3.5562\n",
      "Epoch: 7/10... Step: 15488... Loss: 2.7586... ppl: 35.0393  Val Loss: 3.5565\n",
      "Epoch: 7/10... Step: 15520... Loss: 2.9891... ppl: 34.8679  Val Loss: 3.5516\n",
      "Epoch: 7/10... Step: 15552... Loss: 2.5520... ppl: 34.7684  Val Loss: 3.5487\n",
      "Epoch: 7/10... Step: 15584... Loss: 3.4522... ppl: 34.8623  Val Loss: 3.5514\n",
      "Epoch: 7/10... Step: 15616... Loss: 2.1679... ppl: 34.8515  Val Loss: 3.5511\n",
      "Epoch: 7/10... Step: 15648... Loss: 2.8167... ppl: 34.9636  Val Loss: 3.5543\n",
      "Epoch: 7/10... Step: 15680... Loss: 2.5990... ppl: 35.0010  Val Loss: 3.5554\n",
      "Epoch: 7/10... Step: 15712... Loss: 2.2255... ppl: 34.8512  Val Loss: 3.5511\n",
      "Epoch: 7/10... Step: 15744... Loss: 2.7719... ppl: 34.9306  Val Loss: 3.5534\n",
      "Epoch: 7/10... Step: 15776... Loss: 2.9438... ppl: 34.9751  Val Loss: 3.5546\n",
      "Epoch: 7/10... Step: 15808... Loss: 3.1399... ppl: 34.9001  Val Loss: 3.5525\n",
      "Epoch: 7/10... Step: 15840... Loss: 3.4922... ppl: 34.9687  Val Loss: 3.5545\n",
      "Epoch: 7/10... Step: 15872... Loss: 3.2044... ppl: 35.0365  Val Loss: 3.5564\n",
      "Epoch: 7/10... Step: 15904... Loss: 2.5278... ppl: 35.1185  Val Loss: 3.5587\n",
      "Epoch: 7/10... Step: 15936... Loss: 2.6351... ppl: 34.9704  Val Loss: 3.5545\n",
      "Epoch: 7/10... Step: 15968... Loss: 1.9645... ppl: 35.2950  Val Loss: 3.5637\n",
      "Epoch: 7/10... Step: 16000... Loss: 2.7770... ppl: 34.9096  Val Loss: 3.5528\n",
      "Epoch: 7/10... Step: 16032... Loss: 2.6055... ppl: 34.8147  Val Loss: 3.5500\n",
      "Epoch: 7/10... Step: 16064... Loss: 2.3730... ppl: 35.1716  Val Loss: 3.5602\n",
      "Epoch: 7/10... Step: 16096... Loss: 2.2255... ppl: 34.8286  Val Loss: 3.5504\n",
      "Epoch: 7/10... Step: 16128... Loss: 2.4593... ppl: 34.9306  Val Loss: 3.5534\n",
      "Epoch: 7/10... Step: 16160... Loss: 2.6178... ppl: 35.1292  Val Loss: 3.5590\n",
      "Epoch: 7/10... Step: 16192... Loss: 2.4532... ppl: 35.2750  Val Loss: 3.5632\n",
      "Epoch: 7/10... Step: 16224... Loss: 3.1599... ppl: 34.8437  Val Loss: 3.5509\n",
      "Epoch: 7/10... Step: 16256... Loss: 2.4598... ppl: 34.9392  Val Loss: 3.5536\n",
      "Epoch: 7/10... Step: 16288... Loss: 2.8574... ppl: 34.9027  Val Loss: 3.5526\n",
      "Epoch: 7/10... Step: 16320... Loss: 2.3729... ppl: 35.3424  Val Loss: 3.5651\n",
      "Epoch: 7/10... Step: 16352... Loss: 2.4973... ppl: 35.1898  Val Loss: 3.5608\n",
      "Epoch: 7/10... Step: 16384... Loss: 3.3854... ppl: 34.7734  Val Loss: 3.5489\n",
      "Epoch: 8/10... Step: 16416... Loss: 3.2322... ppl: 34.9027  Val Loss: 3.5526\n",
      "Epoch: 8/10... Step: 16448... Loss: 2.6852... ppl: 35.2849  Val Loss: 3.5635\n",
      "Epoch: 8/10... Step: 16480... Loss: 2.6874... ppl: 34.9511  Val Loss: 3.5539\n",
      "Epoch: 8/10... Step: 16512... Loss: 1.6967... ppl: 35.0296  Val Loss: 3.5562\n",
      "Epoch: 8/10... Step: 16544... Loss: 2.6423... ppl: 35.1520  Val Loss: 3.5597\n",
      "Epoch: 8/10... Step: 16576... Loss: 2.9295... ppl: 35.0165  Val Loss: 3.5558\n",
      "Epoch: 8/10... Step: 16608... Loss: 2.8462... ppl: 35.1203  Val Loss: 3.5588\n",
      "Epoch: 8/10... Step: 16640... Loss: 2.3271... ppl: 35.1473  Val Loss: 3.5595\n",
      "Epoch: 8/10... Step: 16672... Loss: 2.7270... ppl: 35.2352  Val Loss: 3.5620\n",
      "Epoch: 8/10... Step: 16704... Loss: 2.7777... ppl: 35.7281  Val Loss: 3.5759\n",
      "Epoch: 8/10... Step: 16736... Loss: 2.9225... ppl: 35.7097  Val Loss: 3.5754\n",
      "Epoch: 8/10... Step: 16768... Loss: 2.7319... ppl: 35.4136  Val Loss: 3.5671\n",
      "Epoch: 8/10... Step: 16800... Loss: 2.2922... ppl: 35.2512  Val Loss: 3.5625\n",
      "Epoch: 8/10... Step: 16832... Loss: 2.3023... ppl: 35.4699  Val Loss: 3.5687\n",
      "Epoch: 8/10... Step: 16864... Loss: 2.6389... ppl: 35.7684  Val Loss: 3.5771\n",
      "Epoch: 8/10... Step: 16896... Loss: 2.2506... ppl: 35.9383  Val Loss: 3.5818\n",
      "Epoch: 8/10... Step: 16928... Loss: 2.5405... ppl: 35.6235  Val Loss: 3.5730\n",
      "Epoch: 8/10... Step: 16960... Loss: 2.4520... ppl: 35.5996  Val Loss: 3.5723\n",
      "Epoch: 8/10... Step: 16992... Loss: 2.5812... ppl: 35.6291  Val Loss: 3.5732\n",
      "Epoch: 8/10... Step: 17024... Loss: 2.3339... ppl: 35.7379  Val Loss: 3.5762\n",
      "Epoch: 8/10... Step: 17056... Loss: 2.5494... ppl: 35.7124  Val Loss: 3.5755\n",
      "Epoch: 8/10... Step: 17088... Loss: 3.2486... ppl: 35.9615  Val Loss: 3.5825\n",
      "Epoch: 8/10... Step: 17120... Loss: 2.6957... ppl: 35.8454  Val Loss: 3.5792\n",
      "Epoch: 8/10... Step: 17152... Loss: 3.1088... ppl: 35.9099  Val Loss: 3.5810\n",
      "Epoch: 8/10... Step: 17184... Loss: 2.5818... ppl: 35.6373  Val Loss: 3.5734\n",
      "Epoch: 8/10... Step: 17216... Loss: 2.4684... ppl: 35.8081  Val Loss: 3.5782\n",
      "Epoch: 8/10... Step: 17248... Loss: 2.3303... ppl: 36.0064  Val Loss: 3.5837\n",
      "Epoch: 8/10... Step: 17280... Loss: 2.3740... ppl: 35.9865  Val Loss: 3.5831\n",
      "Epoch: 8/10... Step: 17312... Loss: 3.3525... ppl: 35.7266  Val Loss: 3.5759\n",
      "Epoch: 8/10... Step: 17344... Loss: 2.6590... ppl: 35.8392  Val Loss: 3.5790\n",
      "Epoch: 8/10... Step: 17376... Loss: 2.7135... ppl: 35.8200  Val Loss: 3.5785\n",
      "Epoch: 8/10... Step: 17408... Loss: 2.6207... ppl: 35.7892  Val Loss: 3.5776\n",
      "Epoch: 8/10... Step: 17440... Loss: 2.5851... ppl: 36.0285  Val Loss: 3.5843\n",
      "Epoch: 8/10... Step: 17472... Loss: 2.7041... ppl: 36.0326  Val Loss: 3.5844\n",
      "Epoch: 8/10... Step: 17504... Loss: 2.7274... ppl: 35.7817  Val Loss: 3.5774\n",
      "Epoch: 8/10... Step: 17536... Loss: 2.7894... ppl: 35.8084  Val Loss: 3.5782\n",
      "Epoch: 8/10... Step: 17568... Loss: 2.7863... ppl: 35.7492  Val Loss: 3.5765\n",
      "Epoch: 8/10... Step: 17600... Loss: 2.7258... ppl: 35.6122  Val Loss: 3.5727\n",
      "Epoch: 8/10... Step: 17632... Loss: 2.5451... ppl: 35.5529  Val Loss: 3.5710\n",
      "Epoch: 8/10... Step: 17664... Loss: 2.5885... ppl: 35.5284  Val Loss: 3.5703\n",
      "Epoch: 8/10... Step: 17696... Loss: 3.1269... ppl: 35.5468  Val Loss: 3.5709\n",
      "Epoch: 8/10... Step: 17728... Loss: 3.0860... ppl: 35.5023  Val Loss: 3.5696\n",
      "Epoch: 8/10... Step: 17760... Loss: 2.5670... ppl: 35.8675  Val Loss: 3.5798\n",
      "Epoch: 8/10... Step: 17792... Loss: 2.4439... ppl: 36.0026  Val Loss: 3.5836\n",
      "Epoch: 8/10... Step: 17824... Loss: 2.6526... ppl: 35.8573  Val Loss: 3.5795\n",
      "Epoch: 8/10... Step: 17856... Loss: 2.3804... ppl: 35.8780  Val Loss: 3.5801\n",
      "Epoch: 8/10... Step: 17888... Loss: 2.8308... ppl: 35.8009  Val Loss: 3.5780\n",
      "Epoch: 8/10... Step: 17920... Loss: 2.2080... ppl: 35.7401  Val Loss: 3.5763\n",
      "Epoch: 8/10... Step: 17952... Loss: 2.7284... ppl: 35.7426  Val Loss: 3.5763\n",
      "Epoch: 8/10... Step: 17984... Loss: 2.6684... ppl: 35.8958  Val Loss: 3.5806\n",
      "Epoch: 8/10... Step: 18016... Loss: 2.7330... ppl: 36.0397  Val Loss: 3.5846\n",
      "Epoch: 8/10... Step: 18048... Loss: 3.0229... ppl: 35.8491  Val Loss: 3.5793\n",
      "Epoch: 8/10... Step: 18080... Loss: 2.9895... ppl: 35.9322  Val Loss: 3.5816\n",
      "Epoch: 8/10... Step: 18112... Loss: 2.2731... ppl: 36.0192  Val Loss: 3.5841\n",
      "Epoch: 8/10... Step: 18144... Loss: 3.3407... ppl: 35.7643  Val Loss: 3.5769\n",
      "Epoch: 8/10... Step: 18176... Loss: 2.8049... ppl: 35.7733  Val Loss: 3.5772\n",
      "Epoch: 8/10... Step: 18208... Loss: 2.6786... ppl: 35.8407  Val Loss: 3.5791\n",
      "Epoch: 8/10... Step: 18240... Loss: 2.7569... ppl: 36.2034  Val Loss: 3.5892\n",
      "Epoch: 8/10... Step: 18272... Loss: 2.0849... ppl: 35.8909  Val Loss: 3.5805\n",
      "Epoch: 8/10... Step: 18304... Loss: 2.8717... ppl: 36.0721  Val Loss: 3.5855\n",
      "Epoch: 8/10... Step: 18336... Loss: 3.0158... ppl: 35.8707  Val Loss: 3.5799\n",
      "Epoch: 8/10... Step: 18368... Loss: 2.8195... ppl: 35.6276  Val Loss: 3.5731\n",
      "Epoch: 8/10... Step: 18400... Loss: 2.7502... ppl: 36.2060  Val Loss: 3.5892\n",
      "Epoch: 8/10... Step: 18432... Loss: 3.0350... ppl: 35.9445  Val Loss: 3.5820\n",
      "Epoch: 8/10... Step: 18464... Loss: 2.4305... ppl: 35.8026  Val Loss: 3.5780\n",
      "Epoch: 8/10... Step: 18496... Loss: 2.1687... ppl: 36.1278  Val Loss: 3.5871\n",
      "Epoch: 8/10... Step: 18528... Loss: 2.8270... ppl: 36.1153  Val Loss: 3.5867\n",
      "Epoch: 8/10... Step: 18560... Loss: 2.3626... ppl: 35.8667  Val Loss: 3.5798\n",
      "Epoch: 8/10... Step: 18592... Loss: 2.8283... ppl: 35.8884  Val Loss: 3.5804\n",
      "Epoch: 8/10... Step: 18624... Loss: 2.8170... ppl: 35.7622  Val Loss: 3.5769\n",
      "Epoch: 8/10... Step: 18656... Loss: 3.4443... ppl: 36.3661  Val Loss: 3.5936\n",
      "Epoch: 8/10... Step: 18688... Loss: 2.2608... ppl: 36.2607  Val Loss: 3.5907\n",
      "Epoch: 8/10... Step: 18720... Loss: 2.5826... ppl: 35.8371  Val Loss: 3.5790\n",
      "Epoch: 9/10... Step: 18752... Loss: 2.4646... ppl: 35.7428  Val Loss: 3.5763\n",
      "Epoch: 9/10... Step: 18784... Loss: 2.4994... ppl: 36.1730  Val Loss: 3.5883\n",
      "Epoch: 9/10... Step: 18816... Loss: 2.8253... ppl: 35.9143  Val Loss: 3.5811\n",
      "Epoch: 9/10... Step: 18848... Loss: 2.8746... ppl: 35.9510  Val Loss: 3.5822\n",
      "Epoch: 9/10... Step: 18880... Loss: 2.7950... ppl: 36.1007  Val Loss: 3.5863\n",
      "Epoch: 9/10... Step: 18912... Loss: 2.9757... ppl: 35.9323  Val Loss: 3.5816\n",
      "Epoch: 9/10... Step: 18944... Loss: 2.6754... ppl: 36.0028  Val Loss: 3.5836\n",
      "Epoch: 9/10... Step: 18976... Loss: 2.7715... ppl: 36.0015  Val Loss: 3.5836\n",
      "Epoch: 9/10... Step: 19008... Loss: 2.9656... ppl: 36.1833  Val Loss: 3.5886\n",
      "Epoch: 9/10... Step: 19040... Loss: 2.5143... ppl: 36.4010  Val Loss: 3.5946\n",
      "Epoch: 9/10... Step: 19072... Loss: 2.5421... ppl: 36.6602  Val Loss: 3.6017\n",
      "Epoch: 9/10... Step: 19104... Loss: 2.4078... ppl: 36.3818  Val Loss: 3.5941\n",
      "Epoch: 9/10... Step: 19136... Loss: 1.9307... ppl: 36.1974  Val Loss: 3.5890\n",
      "Epoch: 9/10... Step: 19168... Loss: 2.3072... ppl: 36.3432  Val Loss: 3.5930\n",
      "Epoch: 9/10... Step: 19200... Loss: 2.8818... ppl: 36.6053  Val Loss: 3.6002\n",
      "Epoch: 9/10... Step: 19232... Loss: 3.0101... ppl: 36.9691  Val Loss: 3.6101\n",
      "Epoch: 9/10... Step: 19264... Loss: 2.8783... ppl: 36.6660  Val Loss: 3.6018\n",
      "Epoch: 9/10... Step: 19296... Loss: 3.0999... ppl: 36.6398  Val Loss: 3.6011\n",
      "Epoch: 9/10... Step: 19328... Loss: 2.9020... ppl: 36.4966  Val Loss: 3.5972\n",
      "Epoch: 9/10... Step: 19360... Loss: 2.8857... ppl: 36.7706  Val Loss: 3.6047\n",
      "Epoch: 9/10... Step: 19392... Loss: 2.4223... ppl: 36.7235  Val Loss: 3.6034\n",
      "Epoch: 9/10... Step: 19424... Loss: 2.8436... ppl: 36.7896  Val Loss: 3.6052\n",
      "Epoch: 9/10... Step: 19456... Loss: 2.5944... ppl: 36.8839  Val Loss: 3.6078\n",
      "Epoch: 9/10... Step: 19488... Loss: 2.5644... ppl: 36.9176  Val Loss: 3.6087\n",
      "Epoch: 9/10... Step: 19520... Loss: 2.6063... ppl: 36.6692  Val Loss: 3.6019\n",
      "Epoch: 9/10... Step: 19552... Loss: 2.1798... ppl: 36.7854  Val Loss: 3.6051\n",
      "Epoch: 9/10... Step: 19584... Loss: 2.2380... ppl: 37.0058  Val Loss: 3.6111\n",
      "Epoch: 9/10... Step: 19616... Loss: 2.7877... ppl: 36.9975  Val Loss: 3.6109\n",
      "Epoch: 9/10... Step: 19648... Loss: 2.8087... ppl: 36.7158  Val Loss: 3.6032\n",
      "Epoch: 9/10... Step: 19680... Loss: 2.5675... ppl: 36.6908  Val Loss: 3.6025\n",
      "Epoch: 9/10... Step: 19712... Loss: 2.3451... ppl: 36.7200  Val Loss: 3.6033\n",
      "Epoch: 9/10... Step: 19744... Loss: 1.9641... ppl: 36.7857  Val Loss: 3.6051\n",
      "Epoch: 9/10... Step: 19776... Loss: 3.1248... ppl: 36.9623  Val Loss: 3.6099\n",
      "Epoch: 9/10... Step: 19808... Loss: 2.0436... ppl: 37.2264  Val Loss: 3.6170\n",
      "Epoch: 9/10... Step: 19840... Loss: 2.3378... ppl: 36.7819  Val Loss: 3.6050\n",
      "Epoch: 9/10... Step: 19872... Loss: 2.7784... ppl: 36.7809  Val Loss: 3.6050\n",
      "Epoch: 9/10... Step: 19904... Loss: 2.4714... ppl: 36.8731  Val Loss: 3.6075\n",
      "Epoch: 9/10... Step: 19936... Loss: 2.4383... ppl: 36.4462  Val Loss: 3.5958\n",
      "Epoch: 9/10... Step: 19968... Loss: 2.5611... ppl: 36.4603  Val Loss: 3.5962\n",
      "Epoch: 9/10... Step: 20000... Loss: 3.1571... ppl: 36.5462  Val Loss: 3.5986\n",
      "Epoch: 9/10... Step: 20032... Loss: 2.8382... ppl: 36.5006  Val Loss: 3.5973\n",
      "Epoch: 9/10... Step: 20064... Loss: 2.4931... ppl: 36.4107  Val Loss: 3.5949\n",
      "Epoch: 9/10... Step: 20096... Loss: 2.6227... ppl: 36.7494  Val Loss: 3.6041\n",
      "Epoch: 9/10... Step: 20128... Loss: 2.7099... ppl: 37.0508  Val Loss: 3.6123\n",
      "Epoch: 9/10... Step: 20160... Loss: 2.8212... ppl: 36.9199  Val Loss: 3.6088\n",
      "Epoch: 9/10... Step: 20192... Loss: 2.2991... ppl: 36.9494  Val Loss: 3.6095\n",
      "Epoch: 9/10... Step: 20224... Loss: 2.4070... ppl: 36.8157  Val Loss: 3.6059\n",
      "Epoch: 9/10... Step: 20256... Loss: 2.3859... ppl: 36.5913  Val Loss: 3.5998\n",
      "Epoch: 9/10... Step: 20288... Loss: 2.8017... ppl: 36.7205  Val Loss: 3.6033\n",
      "Epoch: 9/10... Step: 20320... Loss: 2.6288... ppl: 36.9932  Val Loss: 3.6107\n",
      "Epoch: 9/10... Step: 20352... Loss: 2.3039... ppl: 37.0296  Val Loss: 3.6117\n",
      "Epoch: 9/10... Step: 20384... Loss: 2.7711... ppl: 36.8700  Val Loss: 3.6074\n",
      "Epoch: 9/10... Step: 20416... Loss: 2.5774... ppl: 36.8876  Val Loss: 3.6079\n",
      "Epoch: 9/10... Step: 20448... Loss: 2.4213... ppl: 36.9710  Val Loss: 3.6101\n",
      "Epoch: 9/10... Step: 20480... Loss: 2.5240... ppl: 36.8394  Val Loss: 3.6066\n",
      "Epoch: 9/10... Step: 20512... Loss: 2.6863... ppl: 36.7117  Val Loss: 3.6031\n",
      "Epoch: 9/10... Step: 20544... Loss: 2.5661... ppl: 36.9341  Val Loss: 3.6091\n",
      "Epoch: 9/10... Step: 20576... Loss: 2.5576... ppl: 37.1956  Val Loss: 3.6162\n",
      "Epoch: 9/10... Step: 20608... Loss: 2.5216... ppl: 36.9593  Val Loss: 3.6098\n",
      "Epoch: 9/10... Step: 20640... Loss: 2.3863... ppl: 37.0067  Val Loss: 3.6111\n",
      "Epoch: 9/10... Step: 20672... Loss: 2.7620... ppl: 37.0552  Val Loss: 3.6124\n",
      "Epoch: 9/10... Step: 20704... Loss: 2.8639... ppl: 36.5877  Val Loss: 3.5997\n",
      "Epoch: 9/10... Step: 20736... Loss: 2.5309... ppl: 37.1079  Val Loss: 3.6138\n",
      "Epoch: 9/10... Step: 20768... Loss: 2.8853... ppl: 37.0664  Val Loss: 3.6127\n",
      "Epoch: 9/10... Step: 20800... Loss: 2.8376... ppl: 36.9425  Val Loss: 3.6094\n",
      "Epoch: 9/10... Step: 20832... Loss: 3.0146... ppl: 37.1597  Val Loss: 3.6152\n",
      "Epoch: 9/10... Step: 20864... Loss: 2.3863... ppl: 37.2188  Val Loss: 3.6168\n",
      "Epoch: 9/10... Step: 20896... Loss: 2.6118... ppl: 36.9585  Val Loss: 3.6098\n",
      "Epoch: 9/10... Step: 20928... Loss: 2.5875... ppl: 36.8571  Val Loss: 3.6070\n",
      "Epoch: 9/10... Step: 20960... Loss: 2.6301... ppl: 36.8996  Val Loss: 3.6082\n",
      "Epoch: 9/10... Step: 20992... Loss: 2.2517... ppl: 37.3540  Val Loss: 3.6204\n",
      "Epoch: 9/10... Step: 21024... Loss: 2.3501... ppl: 37.5043  Val Loss: 3.6245\n",
      "Epoch: 9/10... Step: 21056... Loss: 2.6578... ppl: 36.9731  Val Loss: 3.6102\n",
      "Epoch: 10/10... Step: 21088... Loss: 2.7547... ppl: 36.8454  Val Loss: 3.6067\n",
      "Epoch: 10/10... Step: 21120... Loss: 2.4316... ppl: 37.3246  Val Loss: 3.6197\n",
      "Epoch: 10/10... Step: 21152... Loss: 2.6351... ppl: 37.0673  Val Loss: 3.6127\n",
      "Epoch: 10/10... Step: 21184... Loss: 2.6594... ppl: 37.0556  Val Loss: 3.6124\n",
      "Epoch: 10/10... Step: 21216... Loss: 2.3185... ppl: 37.1500  Val Loss: 3.6150\n",
      "Epoch: 10/10... Step: 21248... Loss: 2.4221... ppl: 37.0606  Val Loss: 3.6126\n",
      "Epoch: 10/10... Step: 21280... Loss: 3.2099... ppl: 37.2346  Val Loss: 3.6172\n",
      "Epoch: 10/10... Step: 21312... Loss: 2.6900... ppl: 37.0951  Val Loss: 3.6135\n",
      "Epoch: 10/10... Step: 21344... Loss: 2.6148... ppl: 37.1630  Val Loss: 3.6153\n",
      "Epoch: 10/10... Step: 21376... Loss: 2.2983... ppl: 37.2161  Val Loss: 3.6167\n",
      "Epoch: 10/10... Step: 21408... Loss: 2.6495... ppl: 37.7127  Val Loss: 3.6300\n",
      "Epoch: 10/10... Step: 21440... Loss: 2.6324... ppl: 37.6019  Val Loss: 3.6271\n",
      "Epoch: 10/10... Step: 21472... Loss: 2.4953... ppl: 37.2675  Val Loss: 3.6181\n",
      "Epoch: 10/10... Step: 21504... Loss: 2.5118... ppl: 37.3275  Val Loss: 3.6197\n",
      "Epoch: 10/10... Step: 21536... Loss: 2.3354... ppl: 37.6777  Val Loss: 3.6291\n",
      "Epoch: 10/10... Step: 21568... Loss: 2.6568... ppl: 38.0817  Val Loss: 3.6397\n",
      "Epoch: 10/10... Step: 21600... Loss: 2.3042... ppl: 37.9089  Val Loss: 3.6352\n",
      "Epoch: 10/10... Step: 21632... Loss: 2.4493... ppl: 37.8477  Val Loss: 3.6336\n",
      "Epoch: 10/10... Step: 21664... Loss: 2.5567... ppl: 37.6616  Val Loss: 3.6286\n",
      "Epoch: 10/10... Step: 21696... Loss: 2.8972... ppl: 37.7355  Val Loss: 3.6306\n",
      "Epoch: 10/10... Step: 21728... Loss: 2.3276... ppl: 37.8910  Val Loss: 3.6347\n",
      "Epoch: 10/10... Step: 21760... Loss: 2.1314... ppl: 37.7890  Val Loss: 3.6320\n",
      "Epoch: 10/10... Step: 21792... Loss: 2.2437... ppl: 38.1467  Val Loss: 3.6414\n",
      "Epoch: 10/10... Step: 21824... Loss: 2.4595... ppl: 37.9612  Val Loss: 3.6366\n",
      "Epoch: 10/10... Step: 21856... Loss: 2.5482... ppl: 37.8876  Val Loss: 3.6346\n",
      "Epoch: 10/10... Step: 21888... Loss: 2.2938... ppl: 37.8343  Val Loss: 3.6332\n",
      "Epoch: 10/10... Step: 21920... Loss: 2.9160... ppl: 38.0512  Val Loss: 3.6389\n",
      "Epoch: 10/10... Step: 21952... Loss: 2.8348... ppl: 38.1527  Val Loss: 3.6416\n",
      "Epoch: 10/10... Step: 21984... Loss: 1.9286... ppl: 37.8824  Val Loss: 3.6345\n",
      "Epoch: 10/10... Step: 22016... Loss: 2.7411... ppl: 37.7781  Val Loss: 3.6317\n",
      "Epoch: 10/10... Step: 22048... Loss: 2.1897... ppl: 37.8361  Val Loss: 3.6333\n",
      "Epoch: 10/10... Step: 22080... Loss: 2.8304... ppl: 37.8622  Val Loss: 3.6340\n",
      "Epoch: 10/10... Step: 22112... Loss: 2.0203... ppl: 37.9429  Val Loss: 3.6361\n",
      "Epoch: 10/10... Step: 22144... Loss: 2.3521... ppl: 38.3421  Val Loss: 3.6465\n",
      "Epoch: 10/10... Step: 22176... Loss: 2.2886... ppl: 37.8391  Val Loss: 3.6333\n",
      "Epoch: 10/10... Step: 22208... Loss: 2.7442... ppl: 37.7780  Val Loss: 3.6317\n",
      "Epoch: 10/10... Step: 22240... Loss: 2.5414... ppl: 37.9971  Val Loss: 3.6375\n",
      "Epoch: 10/10... Step: 22272... Loss: 2.7480... ppl: 37.6514  Val Loss: 3.6284\n",
      "Epoch: 10/10... Step: 22304... Loss: 2.7891... ppl: 37.6101  Val Loss: 3.6273\n",
      "Epoch: 10/10... Step: 22336... Loss: 2.5179... ppl: 37.6330  Val Loss: 3.6279\n",
      "Epoch: 10/10... Step: 22368... Loss: 3.0581... ppl: 37.4579  Val Loss: 3.6232\n",
      "Epoch: 10/10... Step: 22400... Loss: 2.4344... ppl: 37.5357  Val Loss: 3.6253\n",
      "Epoch: 10/10... Step: 22432... Loss: 2.3711... ppl: 37.7249  Val Loss: 3.6303\n",
      "Epoch: 10/10... Step: 22464... Loss: 1.8384... ppl: 38.1676  Val Loss: 3.6420\n",
      "Epoch: 10/10... Step: 22496... Loss: 2.4400... ppl: 37.9424  Val Loss: 3.6361\n",
      "Epoch: 10/10... Step: 22528... Loss: 2.8406... ppl: 38.1367  Val Loss: 3.6412\n",
      "Epoch: 10/10... Step: 22560... Loss: 2.4859... ppl: 38.0158  Val Loss: 3.6380\n",
      "Epoch: 10/10... Step: 22592... Loss: 2.3535... ppl: 37.7202  Val Loss: 3.6302\n",
      "Epoch: 10/10... Step: 22624... Loss: 2.8893... ppl: 37.7484  Val Loss: 3.6309\n",
      "Epoch: 10/10... Step: 22656... Loss: 2.4572... ppl: 37.9043  Val Loss: 3.6351\n",
      "Epoch: 10/10... Step: 22688... Loss: 2.8729... ppl: 38.0501  Val Loss: 3.6389\n",
      "Epoch: 10/10... Step: 22720... Loss: 2.3527... ppl: 38.1740  Val Loss: 3.6422\n",
      "Epoch: 10/10... Step: 22752... Loss: 2.8447... ppl: 37.9933  Val Loss: 3.6374\n",
      "Epoch: 10/10... Step: 22784... Loss: 2.7362... ppl: 38.1148  Val Loss: 3.6406\n",
      "Epoch: 10/10... Step: 22816... Loss: 2.5940... ppl: 38.1803  Val Loss: 3.6423\n",
      "Epoch: 10/10... Step: 22848... Loss: 2.8805... ppl: 37.8334  Val Loss: 3.6332\n",
      "Epoch: 10/10... Step: 22880... Loss: 2.1501... ppl: 38.0889  Val Loss: 3.6399\n",
      "Epoch: 10/10... Step: 22912... Loss: 1.9006... ppl: 38.4145  Val Loss: 3.6484\n",
      "Epoch: 10/10... Step: 22944... Loss: 2.6339... ppl: 38.2400  Val Loss: 3.6439\n",
      "Epoch: 10/10... Step: 22976... Loss: 2.5126... ppl: 37.9774  Val Loss: 3.6370\n",
      "Epoch: 10/10... Step: 23008... Loss: 2.6084... ppl: 38.3304  Val Loss: 3.6462\n",
      "Epoch: 10/10... Step: 23040... Loss: 2.3272... ppl: 37.7097  Val Loss: 3.6299\n",
      "Epoch: 10/10... Step: 23072... Loss: 2.5768... ppl: 38.0255  Val Loss: 3.6383\n",
      "Epoch: 10/10... Step: 23104... Loss: 2.4186... ppl: 38.1263  Val Loss: 3.6409\n",
      "Epoch: 10/10... Step: 23136... Loss: 2.3803... ppl: 37.9774  Val Loss: 3.6370\n",
      "Epoch: 10/10... Step: 23168... Loss: 2.5806... ppl: 38.2205  Val Loss: 3.6434\n",
      "Epoch: 10/10... Step: 23200... Loss: 2.2112... ppl: 38.2497  Val Loss: 3.6441\n",
      "Epoch: 10/10... Step: 23232... Loss: 2.3900... ppl: 38.1062  Val Loss: 3.6404\n",
      "Epoch: 10/10... Step: 23264... Loss: 2.4923... ppl: 37.8912  Val Loss: 3.6347\n",
      "Epoch: 10/10... Step: 23296... Loss: 2.4571... ppl: 37.8982  Val Loss: 3.6349\n",
      "Epoch: 10/10... Step: 23328... Loss: 2.4611... ppl: 38.2221  Val Loss: 3.6434\n",
      "Epoch: 10/10... Step: 23360... Loss: 3.0037... ppl: 38.3334  Val Loss: 3.6463\n",
      "Epoch: 10/10... Step: 23392... Loss: 2.5728... ppl: 38.2276  Val Loss: 3.6436\n",
      "Epoch: 10/10... Step: 23424... Loss: 2.1450... ppl: 37.9379  Val Loss: 3.6360\n"
     ]
    }
   ],
   "source": [
    "# specify batch size\n",
    "batch_size = 64\n",
    "\n",
    "# train the model\n",
    "train(net, batch_size = batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQ6leiAm0Wmn"
   },
   "source": [
    "# 6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1396,
     "status": "ok",
     "timestamp": 1596874248978,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "FPoIBPQpgqZy",
    "outputId": "8ee728df-b6cb-4bb9-88cb-c7c4524fce89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1596874485946,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "bch2a0WglQ96"
   },
   "outputs": [],
   "source": [
    "# function to generate one token\n",
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x).long()\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "      inputs = inputs.cuda()\n",
    "\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  if(torch.cuda.is_available()):\n",
    "      p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
    "  \n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1596874664194,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "EQR-euTiFan9"
   },
   "outputs": [],
   "source": [
    "# function to fetch generated sequence\n",
    "def sample(net, size = 2, seed_text='it is'):\n",
    "        \n",
    "    if(torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = seed_text.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in toks:\n",
    "      token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1596874758920,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "VSakRw3SHRv2",
    "outputId": "a15311db-a4b8-4d17-a949-4e5e3df8036a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed text: i want to >> output: i want to order a pizza from pizza hut\n",
      "\n",
      "\n",
      "seed text: how about a cup >> output: how about a cup of coffee from starbucks for me\n",
      "\n",
      "\n",
      "seed text: i don't want >> output: i don't want to drive it home and i\n",
      "\n",
      "\n",
      "seed text: can you send >> output: can you send me the confirmation to my email\n",
      "\n",
      "\n",
      "seed text: my car >> output: my car is making a weird knocking noise\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seed texts\n",
    "seeds = [\"i want to\",\n",
    "         \"how about a cup\",\n",
    "         \"i don't want\",\n",
    "         \"can you send\",\n",
    "         \"my car\"]\n",
    "\n",
    "# number of tokens to generate\n",
    "num_toks = 6\n",
    "\n",
    "# text generation\n",
    "for s in seeds:\n",
    "  # get generated text from the model\n",
    "  text_gen = sample(net, num_toks, seed_text=s)\n",
    "  # print the result\n",
    "  print(\"seed text:\", s, \">> output:\",text_gen)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation using Neural Language Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1eccf91efb3641ab93437844fb3d8515": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "233b3deec9394d288ed235c2054abe08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "433348a8133446d0a9a4dc38f028bdc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "92d7ebd42a4e463eaed43f3960cfe36c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b00386afebe447d989c1d40fc75ebb24",
       "IPY_MODEL_bb3bafbb85f14bd0a9904c4f08d40946"
      ],
      "layout": "IPY_MODEL_1eccf91efb3641ab93437844fb3d8515"
     }
    },
    "b00386afebe447d989c1d40fc75ebb24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c684ec2cc5e9413a822c072387aeedd5",
      "max": 64776,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_433348a8133446d0a9a4dc38f028bdc6",
      "value": 64776
     }
    },
    "bb3bafbb85f14bd0a9904c4f08d40946": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbb68ce201524cfa8eafec85788f9c42",
      "placeholder": "​",
      "style": "IPY_MODEL_233b3deec9394d288ed235c2054abe08",
      "value": " 64776/64776 [00:18&lt;00:00, 3464.93it/s]"
     }
    },
    "c684ec2cc5e9413a822c072387aeedd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbb68ce201524cfa8eafec85788f9c42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
